+++
date = "2017-03-10T19:00:00"
title = "已經發展了 61 年的人工智能，現在面臨着這 6 個挑戰"
titleimage = "http://pic1.zhimg.com/5cf8632dc29429d222c2216304620620.jpg"
ga = 031019
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title"></h2>

<div class="answer">

<div class="content">
<p>回答來自<a href="https://www.zhihu.com/org-intro">機構帳號</a>：<a class="author-link" href="https://www.zhihu.com/org/wei-ruan-ya-zhou-yan-jiu-yuan" target="_blank">微軟亞洲研究院</a><span class="OrgIcon sprite-global-icon-org-14"></span></p>
</div>
</div>


</div>





<div class="question">
<h2 class="question-title">深度學習（機器學習）的下一步如何發展？</h2>

<div class="answer">



<div class="content">
<p>針對這個問題，我們邀請了微軟亞洲研究院<strong>機器學習組</strong>的主管研究員秦濤博士與大家分享他的觀點。</p>
<p>要回答這個問題，先要從<strong>人工智能近年的進展</strong>開始說起。</p>
<p>從 1956 年達特茅斯會議上人工智能的誕生開始，到如今人工智能已經發展了 61 年，這期間人工智能歷經風雨，經歷了數次高潮也有數次低谷，每次高潮都是因爲核心技術的提出引起了人們極大的興趣，吸引了大量的資金的投入。但同時由於大家的期望值遠遠超過了技術所能夠達到的高度，因此當人們發現巨大的資金和人才的投入不能達到預期成果的時候，人工智能的冬天也隨之而來。幸運的是，現在我們正處於人工智能的第三次浪潮，並且目前看來，距離下一個冬天還是挺遠的。從媒體的報道，大家可能都能瞭解到，人工智能在各個方向都取得了非常大的進展，不管是研究上、實踐上，還是應用上。下面我們簡單回顧一下人工智能近年來在各個方向取得的進展。</p>
<p>早在 2012 年，微軟就在&ldquo;21 世紀的計算&rdquo;大會上展示了一個同聲傳譯的系統，這個系統其實相當複雜：當微軟研究院創始人 Rick Rashid 用英文演講的時候，這個系統首先需要將英文的語音識別成英文的文本，然後通過一個翻譯系統把英文翻譯成中文，然後再把中文文本合成成爲中文的語音。整個複雜的過程都是通過深度學習的技術來支撐的。</p>
<p>在 2015 年底，發生了一件對計算機視覺領域而言非常重要的事情，就是微軟亞洲研究院的研究員提出了一個新的基於 CNN 的深度模型叫做殘差網絡，這個殘差網絡深度高達 152 層，取得了當時圖象識別比賽上面最好的成績。到現在爲止，深度殘差網絡在計算機視覺的研究中被廣泛使用，並且被集成到微軟還有其他大公司的產品中。</p>
<p>再到後來，2016 年初，可能大家都知道，AlphaGo 這個系統打敗了圍棋世界冠軍李世石，這非常出乎人們的預料，特別是 AI 專家的預料，因爲大家普遍認爲，機器要在圍棋上戰勝人類可能還需要 20 年。在 2016 年下半年，微軟宣佈了另外一項 AI 上的進展，就是在日常對話的語音識別中，微軟的技術已經達到了人類的水平，這也是非常了不起的，因爲如果大家關注一下我們日常的講話，就會發現，其中有很多停頓，並且帶一些語氣詞，與朗誦或者新聞播音相差很大，這種日常對話識別要達到人類的水平是很不容易的。</p>
<p>從以上的簡單回顧可以看出，<strong>人工智能的第三波浪潮和深度學習是分不開的。</strong>深度學習裏最經典的模型是全連接的神經網絡，就是每相臨的兩層之間節點之間是通過邊全連接；再就是卷積神經網絡，這個在計算機視覺裏面用得非常多；再就是循環神經網絡 RNN，這個在對系列進行建模，例如自然語言處理或者語音信號裏面用得很多，這些都是非常成功的深度神經網絡的模型。還有一個非常重要的技術就是深度強化學習技術，這是深度學習和強化學習的結合，也是 AlphaGo 系統所採用的技術。</p>
<p>深度學習的成功主要歸功於三大因素&mdash;&mdash;<strong>大數據、大模型、大計算</strong>。現在可以利用的數據特別是人工標註的數據非常多，使得我們能夠從數據中學到以前沒法學習的東西。另外技術上的發展使得訓練大模型成爲了可能，例如上千層的深度神經網絡，這個在四年以前都覺得不能想象的事情，現在都已經發展成爲現實，並且在產品中都有了很廣泛的使用。再就是大計算，從 CPU 到 GPU，可獲取的計算資源越來越豐富。</p>
<p>大數據、大模型、大計算是深度學習的三大支柱，因此這三個方向都是當前研究的熱點，例如如何從更多更大的數據裏面進行學習，如何訓練更大更深的模型。非常深的模型，當前更成功的例子是在計算機視覺裏面，但如何把這種更深的模型引入到自然語言處理裏面，還需要研究，例如當前幾個大公司的神經機器翻譯模型，都是利用較深的 RNN，但是還是遠遠達不到殘差網絡的深度。從大計算這個方面來講，整個演變過程是從 CPU 到 GPU 到 FPGA，再發展到現在有些公司定製自己專有芯片，國內的有一些創業公司，也都在做一些 AI 芯片，專門爲 AI 來設計一些硬件。大計算另外一個角度就是深度學習的平臺和系統，這個可以說是各大 AI 或者是互聯網公司的着重發力的地方，例如微軟的 CNTK、DMTK，再比如 TensorFlow、Torch，以及學術界的開源平臺包括 Theano、Caffe、MxNet 等等。可以預計，在短期內，各大公司還會在這個領域做非常激烈的競爭，希望能夠吸引第三方公司使用他們的平臺和系統。</p>
<p><strong>俗話說成也蕭何敗也蕭何，大數據、大模型、大計算是深度學習成功的三大支柱因素，但他們同時也爲深度學習的進一步發展和普及帶來了一些制約因素。接下來，我會爲大家介紹目前深度學習的五大挑戰及其解決方案。</strong></p>
<p><strong>挑戰 1：標註數據代價昂貴</strong></p>
<p><strong>前沿 1：從無標註的數據裏學習</strong></p>
<p>大家都知道，深度學習訓練一個模型需要很多的人工標註的數據。例如在圖象識別裏面，經常我們可能需要上百萬的人工標註的數據，在語音識別裏面，我們可能需要成千上萬小時的人工標註的數據，機器翻譯更是需要數千萬的雙語句對做訓練，在圍棋裏面 DeepMind 當初訓練這個模型也用了數千萬圍棋高手走子的記錄，這些都是大數據的體現。</p>
<p>但是，很多時候找專家來標註數據是非常昂貴的，並且對一些應用而言，很難找到大規模的標註的數據，例如一些疑難雜症，或者是一些比較稀有的應用場景。這裏我們做一個粗略的分析，看看標註數據的代價有多高。比如說對機器翻譯而言，現在如果我們請人工來翻譯，一個單詞的費用差不多是 5&mdash;10 美分之間，一個句子平均長度差不多是 30 個單詞，如果我們需要標註一千萬個雙語句對，也就是我們需要找專家翻譯一千萬句話，這個標註的費用差不多是 2200 萬美元。</p>
<p>大家可以看到數據標註的費用是非常非常高的，讓一個創業公司或者一些剛剛涉足人工智能的公司拿這麼大一筆資金來標註數據是很難或者是不太可行的。因此當前深度學習的一個前沿就是如何從無標註的數據裏面進行學習。現在已經有相關的研究工作，包括最近比較火的生成式對抗網絡，以及我們自己提出的對偶學習。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-5ab02d8989a5e5894c6bcf5060d9b4f9_b.jpg" alt="">
<p>生成式對抗網絡的主要目的是學到一個生成模型，這樣它可以生成很多圖像，這種圖像看起來就像真實的自然圖像一樣。它解決這個問題的思路跟以前的方法不太一樣，它是同時學習兩個神經網絡：一個神經網絡生成圖像，另外一個神經網絡給圖像進行分類，區分真實的圖像和生成的圖像。在生成式對抗網絡裏面，第一個神經網絡也就是生成式神經網絡，它的目的是希望生成的圖像非常像自然界的真實圖像，這樣的話，那後面的第二個網絡，也就是那個分類器沒辦法區分真實世界的圖像和生成的圖像；而第二個神經網絡，也就是分類器，它的目的是希望能夠正確的把生成的圖像也就是假的圖像和真實的自然界圖像能夠區分開。大家可以看到，這兩個神經網絡的目的其實是不一樣的，他們一起進行訓練，就可以得到一個很好的生成式神經網絡。生成式對抗網絡最初提出的時候，主要是對於圖像的生成，現在很多人把他應用到各個不同的問題上，包括自然語言理解，比如說最近我們有一個工作，就是把這種思想應用到機器翻譯裏面，能夠很大幅度的提高機器翻譯的準確度。</p>
<p>針對如何從無標註的數據進行學習，我們組裏面提出了一個新思路，叫做對偶學習。對偶學習的思路和前面生成式對抗學習會非常不一樣。對偶學習的提出是受到一個現象的啓發：我們發現很多人工智能的任務在結構上有對偶屬性。比如說在機器翻譯裏面，我們把中文翻譯成英文，這是一個任務，但是我們同樣也需要把英文翻譯成中文，這是一個對偶的任務。這種原任務和對偶任務之間，他們的輸入和輸出正好是反着來的。在語音處理裏面，語音識別是把語音轉化成文字，語音合成是把文字轉化成語音，也是互爲對偶的兩個任務。在圖像理解裏面，看圖說話，也就是給一張圖生成一句描述性的語句，它的對偶任務是給一句話生成一張圖，這兩個任務一個是從圖像到文本，另外一個是從文本到圖像。在對話系統裏面，回答問題和問題生成也是互爲對偶的兩個問題，前者是給定問題生成答案，後者是給定答案生成問題。在搜索引擎裏面，給定檢索詞返回相關文檔和給定文檔或者廣告返回關鍵詞也是互爲對偶的問題：搜索引擎最主要的任務是針對用戶提交的檢索詞匹配一些文檔，返回最相關的文檔；當廣告商提交一個廣告之後，廣告平臺需要給他推薦一些關健詞使得他的廣告在用戶搜索這些詞能夠展現出來被用戶點擊。</p>
<img class="content-image" src="http://pic3.zhimg.com/70/v2-2510829f19e6696cb9cc358643df3e5e_b.jpg" alt="">
<p>對偶學習試圖把這種結構的對偶屬性應用在機器學習裏。其基本思想比較簡單，我們以機器翻譯爲例子來說明。我們想把一箇中文句子翻譯成英文，我們可以先用一箇中文到英文的翻譯模型，把這個句子翻譯成英文的句子，因爲我們沒有英文的標註，所以不知道這個英文的翻譯是好還是壞以及有多好多壞。我們再利用從英文到中文的翻譯模型，把這個英文的句子翻譯成中文的句子，這樣一來，我們就得到了一個新的中文句子。整個過程包含了正向翻譯和反向翻譯互爲對偶的兩個步驟。然後我們比較原始中文的句子和後來得到的中文句子，如果兩個翻譯模型都很好的話，這兩個中文的句子應該比較相似，如果兩個模型不好或者有一個模型不好的話，得到的兩個中文句子就不相似。因此我們可以通過這種對偶過程從無標註的數據獲得反饋信息，知道我們的模型工作的好還是不好，進而根據這些反饋信息來訓練更新正向反向模型，從而達到從無標註數據學習的目的。</p>
<p>我們在機器翻譯裏面做了一些實驗，發現通過對偶學習的過程，我們只需要用 10%標註的數據（大概 100 萬英法雙語句對），再加上很多沒有標註的數據，達到用 100%標註數據（1200 萬英法雙語句對）訓練的模型的準確度。大家回想一下，我們前面有個粗略的估計，一千萬個訓練語料標註的費用差不多 2200 萬美元，如果我們能把標註的人工費用從 2200 萬美元降到 200 萬美元，這會是一個非常好的結果，能夠大大降低公司運營成本提高運營效率。</p>
<p>最近我們在對偶學習的研究上有一些新的進展，把對偶學習這種基本思想應用到其他的問題裏面，像圖像分類、圖像生成，以及對自然語言的情感分析。我們發現這種結構的對偶屬性可以從不同角度幫助機器學習，提高學習算法的準確度。</p>
<p>從無標註的數據進行學習，我們預計在未來三到五年還是非常重要的一個問題，並且對我們實際的應用也會有很大的幫助。很多問題以前是因爲受限於沒有標註的數據，沒有辦法用深度學習技術，如果我們能夠從無標註的數據進行學習，那麼很多應用很多問題裏面都可以應用深度學習技術。</p>
<p><strong>挑戰 2：大模型不方便在移動設備上使用</strong></p>
<p><strong>前沿 2：降低模型大小</strong></p>
<p>現在常見的模型，像圖像分類裏面，微軟設計的深度殘差網絡，模型大小差不多都在 500M 以上。自然語言處理的一些模型，例如語言模型（language modeling）隨着詞表的增長而變大，可以有幾 G、幾十 G 的大小，機器翻譯的模型也都是 500 兆以上。當然 500M 的大小大家可能覺得沒有多大，一個 CPU 服務器很容易就把這個模型給 load 進去使用。但是大家要注意到，很多時候深度學習的模型需要在一些移動設備上使用。比如說手機輸入法，還有各種對圖像做變換做處理做藝術效果的 app，如果使用深度學習的話效果會非常好，但是這種模型由於它們的 size 太大，就不太適合在手機上應用。大家可以設想一下，如果一個手機的 app 需要加載一個 500M 甚至 1G 以上的模型恐怕不太容易被用戶接受。</p>
<p>因此當前深度學習面臨的第二個挑戰就是如何把大模型變成小模型，這樣可以在各種移動設備上使用。因爲移動設備不僅僅是內存或者存儲空間的限制，更多是因爲能耗的限制，不允許我們用太大的模型。近兩年來，有一些相應的工作，今天我主要介紹兩種：第一種是針對計算機視覺裏面的 CNN 模型，也就是卷積神經網絡，做模型壓縮；第二種是我們去年做的，針對一些序列模型或者類似自然語言處理的 RNN 模型如何做一個更巧妙的算法，使得它模型變小，並且同時精度沒有損失。</p>
<ul>
<li><strong>通過模型壓縮的技術縮減模型的大小</strong></li>
</ul>
<p>對卷積神經網絡而言，近一兩年有一些項目，主要是採用模型壓縮的技術縮減模型的大小。模型壓縮的技術，可以分爲四類：</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-c97012815589d935a5220a6525ed80a9_b.jpg" alt="">
<p>一個是叫<strong>剪枝</strong>，大家知道，神經網絡主要是由一層一層的節點通過邊連接，每個邊上有些權重。剪枝的意思很簡單，如果我們發現某些邊上的權重很小，這樣的邊可能不重要，這些邊就可以去掉。我們在把大模型訓練完之後，看看哪些邊的權重比較小，把這些邊去掉，然後在保留的邊上重新訓練模型；</p>
<p>模型壓縮的另外一種做法就是通過<strong>權值共享</strong>。假設相鄰兩層之間是全連接，每層有一千個節點，那麼這兩層之間有一千乘一千也就是一百萬個權值（參數）。我們可以對一百萬個權值做個聚類，看看哪些權值很接近，我們可以用每個類的均值來代替這些屬於這一類的權值，這樣很多邊（如果他們聚在同一類）共享相同的權值。如果我們把一百萬個數聚成一千類，就可以把參數的個數從一百萬降到一千個，這也是一個非常重要的一個壓縮模型大小的技術。</p>
<p>還有一個技術可以認爲是權值共享的更進一步，叫<strong>量化</strong>。深度神經網絡模型的參數都是用的浮點型的數表達，32bit 長度的浮點型數。實際上沒必要保留那麼高的精度，我們可以通過量化，比如說就用 0 到 255 表達原來 32 個 bit 所表達的精度，通過犧牲精度來降低每一個權值所需要佔用的空間。</p>
<p>這種量化的更極致的做法就是第四類的技術，叫<strong>二制神經網絡</strong>。所謂二制神經網絡，就是所有的權值不用浮點數表達了，就是一個二進制的數，要麼是 +1 要麼是 -1，用二進制的方式來表達，這樣原來一個 32 bit 權值現在只需要一個 bit 來表達，從而大大降低這個模型的尺寸。</p>
<img class="content-image" src="http://pic1.zhimg.com/70/v2-761342a9192893b2f3158c079c64a528_b.jpg" alt="">
<p>上面這張圖顯示了多種模型壓縮的技術在不同卷積神經網絡上的結果。我們可以看到，隨着原始網絡大小的不同，得到的壓縮比是不一樣的，特別是 VGGNet，一個非常重要的卷積神經網絡，能夠把大小從原來的 550M 壓縮到 11M，並且讓人驚奇的是，壓縮後分類的準確率沒有下降，反而略微有一點提高，這是非常了不起的。</p>
<ul>
<li><strong>通過設計更精巧的算法來降低模型大小</strong></li>
</ul>
<p>下面簡單提一下我們組是如何對一些序列模型進行壓縮，也就是對循環神經網絡 RNN 做壓縮，我們提了一種新的循環神經網絡叫做 LightRNN，它不是通過模型壓縮的方式降低模型的大小，而是通過設計一種更精巧的算法來達到降低模型大小。</p>
<p>自然語言相關的應用中，模型之所以大，是因爲我們需要把每一個詞要做詞嵌入（word embedding），把每一個單詞表達成向量空間的一個向量。詞嵌入的基本思想是，語義相似或相近的詞在向量空間裏面的向量也比較接近，這樣就可以通過向量空間表達詞之間的語義信息或者是相似性。因爲通常我們的詞表會很大，比如說在輸入法裏面，可能詞表需要說上百萬。如果我們詞表有上百萬的詞，每個詞如果是用一千維的一個向量來表達，這個大小就是差不多是一百萬乘以一千再乘以 4 Byte（用 32 位的浮點數來表達），詞嵌入向量的總體大小差不多就有 4G 左右，所以整個 RNN 模型是非常大的。搜索引擎的詞表有上千萬的詞，僅僅詞嵌入向量這部分大小就有 40G 左右，考慮到輸入的詞嵌入和輸出的詞嵌入，整個詞嵌入的大小有 80G 左右了，這麼大的模型很難加載到 GPU 上訓練模型和使用，更不用說放在移動設備上使用。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-76311529f48c74b119e9bf5749849fc9_b.jpg" alt="">
<p>我們的算法的基本思想是：不是用一個向量來表達一個詞，而是用兩個向量表達一個詞，一個行向量 + 一個列向量，不同的詞之間共享行或列向量。我們用一個二維的表格來表達整個詞表，假設這個二維的表格有一千行一千列，這個表格可以表達一百萬個詞；這個表格的每一行有一個行向量，每一列有一個列向量，這樣整個二維表格只需要兩千個向量。如果一個詞（January）在第一行第一列的話，它就由行向量 X1 和列向量 Y1 來聯合表達。考慮一個有一百萬個詞的詞表，原來需要一百萬個嵌入向量，通過這樣一個二維或者是兩個 component 的表格詞嵌入，現在我們只需要一千個行向量和一千個列向量來進行表達，這樣大大降低詞嵌入向量模型的大小。</p>
<p>我們在很多公共的數據集上做測試，結果表明我們提出的 LightRNN 算法極大的減小了模型的尺寸，可以把原來語言模型的大小從 4G 降到 40M 左右，當這個模型只有 40 兆的時候，很容易使得我們在移動設備或者是 GPU 上使用。我們的方法使得深度模型在各種能耗比較低或者內存比較小的設備上的使用成爲了可能。並且我們還發現，通過這樣一種共享的二維詞表的嵌入，我們得到的循環神經網絡模型的精度並沒有受到很大的影響，實際上 LightRNN 的精度反而略微有上升，和前面的卷積神經網絡壓縮的結果比較類似。</p>
<p><strong>挑戰 3：大計算需要昂貴的物質、時間成本</strong></p>
<p><strong>前沿 3：全新的硬件設計、算法設計、系統設計</strong></p>
<p>大計算說起來容易，其實做起來非常不容易，非常不簡單。我們微軟亞洲研究院研究員提出深度殘差網絡，這種網絡如果在 ImageNet 這樣一個上百萬的數據上進行訓練的話，用四塊現在最先進的 GPU 卡 K80 學習訓練時間大概要三週。最近百度做的神經機器翻譯系統，他們用了 32 塊 K40 的 GPU 用了十天做訓練，谷歌的機器翻譯系統用了更多，用了 96 塊 K80 的 GPU 訓練了六天。大家可能都知道 AlphaGo， 它也需要非常大量的計算資源。AlphaGo 的模型包含一個策略神經網絡，還有一個值網絡，這兩個都是卷積神經網絡。它的策略網絡用了 50 塊 GPU 做訓練，訓練了 3 個周，值網絡也是用了 50 塊 GPU，訓練了一週，因此它整個的訓練過程用了 50 塊 CPU 四周時間，差不多一個月。大家可以想一想，如果訓練一個模型就要等一個月，並且我們經常要調各種超參數，一組超參數得到的結果不好，換另外一組超參數，可能要嘗試很多組超參數，如果我們沒有大量的計算資源，一等就是一個月，這從產品的更新換代還有技術創新的角度而言，都不能接受。剛纔說了只是 AlphaGo 訓練的複雜度，其實它的測試，比如說比賽的時候，複雜度也非常高， AlphaGo 的單機版和人下棋的時候，每次下棋需要用 48 塊 CPU 8 塊 GPU，它的分佈式版本就用的更多，每次需要用 1200 塊 CPU 再加上 176 塊 GPU。大家可以想一想，地球上有幾個公司能承受這麼高昂的代價來做深度學習。</p>
<p>因此我們認爲，深度學習所面臨的第三個挑戰是如何設計一些更高級的算法，更快的算法，更有效的算法。手段可能是通過一些全新的硬件設計或者是全新的算法設計，或者是全新的系統設計，使得這種訓練能夠大大的加速。如果我們還是這種訓練動不動就要幾十塊 GPU 或者幾百塊 GPU，要等幾個星期或者是幾個月的話，對工業界和學術界而言都不是好事，我們需要更快速更有效的訓練方法。</p>
<p><strong>挑戰 4：如何像人一樣從小樣本進行有效學習？</strong></p>
<p><strong>前沿 4：數據 + 知識，深度學習與知識圖譜、邏輯推理、符號學習相結合</strong></p>
<p>現在的深度學習主要是從大數據進行學習，就是我給你很多標註的數據，使用深度學習算法學習得到一些模型。這種學習方式和人的智能是非常不一樣的，人往往是從小樣本進行學習。人對圖像進行分類，如果人想知道一個圖像是不是蘋果，只需要很少幾個樣本就可以做到準確分類。兩三歲小孩，開始認識世界的時候，他如果想知道什麼樣的動物是狗，我們給他看幾張狗的圖片，並且告訴他狗有什麼特徵，和其他動物像貓或者羊有什麼區別的話，小孩可以很快很準確的識別狗。但是在 ImageNet 比賽裏，像深度殘差神經網絡，一般來說一個類別大概需要上千張圖片才能進行比較充分的訓練，得到比較準確的結果。還有一個例子就是汽車駕駛，一般來說，通過在駕校的培訓，也就是幾十個小時的學習，幾百公里的練習，大多數人就可以開車上路了，但是像現在的無人車可能已經行駛了上百萬公里，還是達不到人的全自動駕駛的水平。原因在於，人經過有限的訓練，結合規則和知識能夠應付各種複雜的路況，但是當前的 AI 還沒有邏輯思考、聯想和推理的能力，必須靠大數據來覆蓋各種可能的路況，但是各種可能的路況幾乎是無窮的。</p>
<p>前面提到的小孩子認識世界的過程，很多時候，大人可以把一些經驗或者是知識傳授給他們，比如說蘋果是圓形的，有紅色的或者青的蘋果，狗和貓的區別在什麼地方。這種知識很容易通過語言進行傳授，但是對於一個 AI 或者對於一個深度學習算法而言，如何把這種知識轉化成實際模型的一部分，怎麼把數據和知識結合起來，提高模型的訓練的速度或者是識別的精度，這是一個很複雜的問題。</p>
<p>現在我們組有同事正在做這方面的嘗試和努力，我們希望把深度學習、知識圖譜、邏輯推理、符號學習等等結合起來，希望能夠進一步推動人工智能的發展，使人工智能更接近人的智能。</p>
<img class="content-image" src="http://pic1.zhimg.com/70/v2-ea6591c99e726c83b9a101a1cd7189a0_b.jpg" alt="">
<p>今年的人工智能國際頂級會議 AAAI 2017 的最佳論文獎，頒給了一個利用物理或者是一些領域的專業知識來幫助深度神經網絡做無標註數據學習的項目。論文裏的具體例子是上面這張圖裏面一個人扔枕頭的過程，論文想解決的問題是從視頻裏檢測這個枕頭，並且跟蹤這個枕頭的運動軌跡。如果我們沒有一些領域的知識，就需要大量的人工標註的數據，比如說把枕頭標註出來，每幀圖像的哪塊區域是枕頭，它的軌跡是什麼樣子的。實際上因爲我們知道，枕頭的運動軌跡應該是拋物線，二次型，結合這種物理知識，我們就不需要標註的數據，能夠把這個枕頭給檢測出來，並且把它的軌跡準確的預測出來。這篇論文之所以獲得了最佳論文獎，也是因爲它把知識和數據結合起來，實現了從無標註數據進行學習的可能。</p>
<p><strong>挑戰 5：如何從認知性的任務擴展到決策性任務？</strong></p>
<p><strong>前沿 5：博弈機器學習</strong></p>
<p>人的智能包含了很多方面，最基本的階段是認知性智能，也就是對整個世界的認知。我們看到一幅圖能知道里面有什麼，我們聽到一句話知道在說文字。現在對於圖象識別、語音識別，AI 已經差不多能達到人類的水平，當然可能是在某些特定的約束條件下，能夠達到人類的水平。但是其實這種認知性的任務，對人類而言都是非常簡單的，比如說一個三五歲的小孩子已經能做得很好了，現在 AI 所能做的這種事情或者能達到的水平，人其實也很容易做到，只是 AI 可能在速度上更快，並且規模上去之後成本更低，並且 24 小時都不需要休息。更有挑戰的問題是，人工智能能不能做一些人類做不了或者是很難做好的事情。</p>
<p>像圖象識別、語音識別這類認知性的任務，AI 之所以做得好，是因爲這些任務是靜態的，所謂靜態就是給定輸入，預測結果不會隨着時間改變。但是決策性問題，往往和環境有很複雜的交互，在某些場景裏面，如何做最優決策，這些最優決策往往是動態的，會隨着時間改變。</p>
<p>現在有人嘗試把 AI 用到金融市場，例如如何用 AI 技術來分析股票，預測股票漲跌，對股票交易給出建議，甚至是代替人來進行股票交易，這類問題就是動態決策性問題。同樣一支股票同樣的價格，在一週前可能是值得買入，但是一週之後可能就要賣出了，同樣一個事件或者是政治新聞比如說是在總統大選之前發生還是之後發生，對股票市場的影響也完全不一樣。所以決策問題的一個難點就在於時變性。</p>
<p>決策性問題的第二個難點在於各種因素相互影響，牽一髮而動全身。一支股票的漲跌會對其他股票產生影響，一個人的投資決策，特別是大的機構的投資決策，可能會對整個市場產生影響，這就和靜態的認知性任務不一樣的。在靜態認知性任務我們的預測結果不會對問題（例如其他的圖像或者語音）產生任何影響，但是在股票市場，任何一個決定，特別是大的機構的投資策略會對整個市場產生影響，對別的投資者產生影響，對將來會產生影響。無人駕駛某種程度上也是比較類似的，一輛無人車在路上怎麼行駛，是由環境和很多車輛共同決定的，當我們通過 AI 來控制一輛車的時候，我們需要關注周圍的車輛，因爲我們要考慮到周圍的車輛對於當前這個無人車的影響，以及我們無人車（如左轉右轉或者併線）對周圍車輛的影響。</p>
<p>當前深度學習已經在靜態任務裏面取得了很大的成功，如何把這種成功延續和擴展到這種複雜的動態決策問題中，也是當前一個深度學習的挑戰之一。我們認爲，一個可能的思路是博弈機器學習。在博弈機器學習裏，通過觀察環境和其他個體的行爲，對每個個體構建不同的個性化行爲模型，AI 就可以三思而後行，選擇一個最優策略，該策略會自適應環境的變化和其他個體的行爲的改變。</p>
<p>最後，我們做一個簡單的總結，在我們看來，當前深度學習的前沿（也是面臨的挑戰）有以下幾個方面，<strong>一個是如何從大量的無標註的數據進行學習，二是如何得到一些比較小的模型使得深度學習技術能夠在移動設備和各種場所裏面得到更廣泛的應用，三是如何設計更快更高效的深度學習算法，四是如何把數據和知識結合起來，五是如何把深度學習的成功從一些靜態的任務擴展到複雜的動態決策性任務上去。</strong>實際上深度學習還有其他一些前沿研究方向，例如如何自主學習（自主學習超參數、網絡結構等）以及如何實現通用人工智能等等，限於時間，不能一一介紹。感興趣的知友們可以自行查閱相關論文。</p>
</div>
</div>




</div>





<div class="question">
<h2 class="question-title"></h2>

<div class="answer">

<div class="content">
<p>「知乎<span class="lG">機構</span><span class="lG">帳號</span>」是<span class="lG">機構</span>用戶專用的知乎<span class="lG">帳號</span>，與知乎社區內原有的個人<span class="lG">帳號</span>獨立並行，其使用者爲有正規資質的組織<span class="lG">機構</span>，包括但不限於科研院所、公益組織、政府機關、媒體、企業等。這不僅是知乎對<span class="lG">機構</span>的「身份認證」，更是涵蓋了內容流通機制、<span class="lG">帳號</span>規範等全套<span class="lG">帳號</span>體系。和個人<span class="lG">帳號</span>一樣，<span class="lG">機構</span><span class="lG">帳號</span>開通不需要任何費用，同時也受社區規範的監督管理，並要遵守相關協議。目前<span class="lG">機構</span><span class="lG">帳號</span>入駐採用邀請制。您可以通過 &nbsp;<a href="http://zhihu.com/org-intro" target="_blank">什麼是「知乎機構帳號」</a>&nbsp;來了解更多<span class="lG">機構</span><span class="lG">帳號</span>信息。</p>
</div>
</div>


</div>


</div>
</div>