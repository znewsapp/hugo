+++
date = "2017-03-28T18:00:00"
title = "認真學習了，但見過的題會做，沒見過的就不會做"
titleimage = "https://pic1.zhimg.com/v2-03a10de7f9098ebc58f3b29471b23248.jpg"
ga = 032818
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">人腦有海量的神經元（參數），那麼人腦有沒有「過擬合」行爲？</h2>

<div class="answer">



<div class="content">
<p>根據人們對過擬合的理解不同，答案也不同。這裏不談過擬合的原因，僅談我理解的過擬合的表現：訓練集上表現優秀，測試集上表現欠佳；換通俗的話說，已見過的題目會解答，未見過的題目解不開。</p>
<p>不同意上述對過擬合描述的朋友不需要往下看了。</p>
<p><strong>語音：</strong></p>
<ul>
<li>聽不懂其他地區人的同國話：</li>
</ul>
<p><strong>a. </strong>音調 / 頓挫：排除詞彙不同的情況，即便是相同詞彙，很多人聽不懂大連話的抑揚頓挫。同樣現象的有日本的大阪弁，印度和新西蘭等英語。</p>
<p><strong>b. </strong>音節：同樣的發音，在某些人的口中會有些許不同。內容相同時，很多人會聽不懂老年人和小孩的發音。</p>
<p><strong> c. </strong>噪音：透過對講機的話，無法聽清從未遇到過的噪音環境的對話。</p>
<blockquote>這些現象的共性都是大家熟悉（表現良好）舊環境（已見的訓練樣本），而不適應（表現不佳）新環境（未見的測試樣本）所造成的。例子中造成&ldquo;過擬合&rdquo;的具體原因會有不同。</blockquote>
<ul>
<li>其他原因造成的聲音會被識別成&ldquo;合理的聲音&rdquo;</li>
</ul>
<p><strong> a. </strong>有個被洗澡的貓的叫聲就被中國人識別成了&ldquo;巧克力&rdquo;</p>
<p><strong> b. </strong>外語的很多音會被中國人識別成中文音，中文註音 W（打不溜）</p>
<blockquote>這類現象是腦中的神經元的組合只爲完成對中文的識別即可，而這種組合有無數種，構建出還能顧及外語音節的神經元方式的可能性較低。</blockquote>
<p>其他例子</p>
<p><strong>畫面：</strong></p>
<p><strong> 1. </strong>看不懂別人的字體，只能看懂&ldquo;規整&rdquo;的字體</p>
<p><strong> 2. </strong>這張海豚圖對曾經純真的我來說越來越模糊了</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-c214fba8ea30fba035b5bded9cbcaf85_b.jpg" alt="">
<p><strong>題海：</strong>只會做已經做過的題，而不會做沒見過的題。高考前的題海戰術就是在抑制過擬合。</p>
<p><strong>施工：</strong>在熟悉的廚房做飯好吃&hellip;&hellip;</p>
<p>生物學系和機器學習的方式不同，但目的是相同的。</p>
<p>兩者都是從有限的數據中找到可以解釋該數據的映射 f，並且再次使用。</p>
<p>只要數據是有限的，那麼過擬合就無法避免。但反過來，如果可以獲得所有數據，用無限大的查找表就能完美表示該映射，這同時失去了學習的意義。</p>
<blockquote>因爲學習就是要從有限的數據中獲得較好的 f。</blockquote>
<p>過擬合是無法被完全消除的，只能被抑制。大家所說的防止過擬合往往也都是指抑制過擬合。</p>
<p>機器學習和人類學習雖然<strong>實現方式不同</strong>，但<strong>要達成的目的類似</strong>。</p>
<p>機器學習所面臨的問題，人類學習同樣面對。</p>
<p>不過大腦有非常好的克服機制。可以嘗試比較一下：</p>
<p>神經網絡抑制過擬合有以下常用的幾點，而這幾點在人們日常學習中同樣適用。</p>
<blockquote>拿平時做題爲訓練集，高考爲測試集來說。</blockquote>
<ol>
<li>dropout（遺忘），細節有時也能形成規律，但不會每次都形成。遺忘可以去掉那些偶然形成的細節規律，提高普遍性。</li>
<li>shuffle（亂序），訓練的樣本不要有固定順序，而要隨機打亂，同樣可以抑制偶然形成的細節規律。比如不要一直從 abandon 開始背單詞一樣。</li>
<li>L2 regularization（保持最簡化），解決的方案不要過於複雜。不然只能顧及特例而失去普遍性。</li>
<li>mini-batch（多題一起做），相互比較後得出結論。比如同時看兩本描述不同的書可以得到更好地理解。</li>
<li>noisy layer（加噪音），題目加入一些干擾項、改變考前環境、教室、平時狀態等。</li>
</ol>
<p>又如下圖中對於<strong>是否有教師指到的問題上</strong>比較人類學習和深層學習對應的類似點。</p>
<p>a. 人類若有老師，當將問題想偏或想複雜時，老師可以提醒你；當深層學習找出的解過於複雜時，L2 regularization 會產生較高的懲罰。</p>
<p>b. 人類若有老師，可以不用從零開始學習，而知道從哪裏開始；當深層學習預訓練時，同樣可以找到一個較好的起點來避開局部極小值或鞍點。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-c06c2bf088669764517d5c1c7a66d085_b.jpg" alt="">
<p><strong>說深層學習和人腦中的神經網絡不相同，相當於是說計算機在算乘法和人類大腦在算乘法時不相同。二者的實現方式當然不同。當關注點不該在此，而要考慮如何彼此借鑑和指導。</strong></p>
<p>谷歌的 deep mind 難道是覺得好玩才研究 deep dream 嗎？背後的邏輯更可能是既然做夢被自然選擇留下來，就或許有其潛在的對學習有利的作用在其中。</p>
<p><strong>題外話：</strong></p>
<p>學習是爲了再次使用，而再次使用的場景往往會比被學習所用到的內容大無數倍。學習同時也是一個動態過程，因爲應用的場景也會隨時間和需求改變。</p>
<p>這種改變是對於智能而言非常重要的能力之一。人類在這方面做的就比較好。</p>
<p>但對於目前的機器學習而言，這種改變往往意味着重新訓練一個神經網絡，但這樣好時巨大，以前學過的內容全部都白費了。很多研究都是努力解決該問題。這是機器學習正在不斷努力的方向，也同時是想要超越人類的地方。因爲&ldquo;調節&rdquo;能力對於人腦而言同樣非常困難。想想成人從中文學習英語的過程。</p>
</div>
</div>




</div>


</div>
</div>