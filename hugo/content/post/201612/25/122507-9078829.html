+++
date = "2016-12-25T07:00:00"
title = "不堆砌公式，用最直觀的方式帶你入門深度學習"
titleimage = "http://pic2.zhimg.com/fd64f6045330445e46e9debe6b8da875.jpg"
ga = 122507
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">深度學習如何入門？</h2>

<div class="answer">



<div class="content">
<p>因爲近期要做一個關於深度學習入門的技術分享，不想堆砌公式，讓大家聽得一頭霧水不知不覺摸褲兜掏手機刷知乎。所以花了大量時間查資料看論文，有的博客或者論文寫得非常贊，比如三巨頭 LeCun，Bengio 和 Hinton 2015 年在 Nature 上發表綜述論文的&ldquo;<a href="http://link.zhihu.com/?target=http%3A//www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf">Deep Learning</a>&rdquo;，言簡意賅地引用了上百篇論文，但適合閱讀，不適合 presentation 式的分享；再如 Hulk 寫的電子書《神經網絡與深度學習》（<a href="http://link.zhihu.com/?target=http%3A//www.tensorfly.cn/home/">中文版</a>，<a href="http://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/index.html">英文版</a>）通俗易懂，用大量的例子解釋了深度學習中的相關概念和基本原理，但適合於抽兩三天的功夫來細品慢嚼，方能體會到作者的良苦用心；還有 Colah 寫的<a href="http://link.zhihu.com/?target=http%3A//colah.github.io/">博客</a>，每一篇詳細闡明瞭一個主題，如果已經入門，這些博客將帶你進階，非常有趣。</p>
<p>還翻了很多知乎問答，非常贊。但發現很多&rdquo;千贊侯&rdquo;走的是彙總論文視頻教程以及羅列代碼路線，本來想半小時入門卻一腳踏進了汪洋大海；私以爲，這種適合於有一定實踐積累後按需查閱。還有很多&rdquo;百贊戶&rdquo;會拿雞蛋啊貓啊狗啊的例子來解釋深度學習的相關概念，生動形象，但我又覺得有避重就輕之嫌。我想，既然要入門深度學習，得有微積分的基礎，會求導數偏導數，知道鏈式法則，最好還學過線性代數；否則，真的，不建議入門深度學習。</p>
<p>最後，實在沒找到我想要的表達方式。我想以圖的方式概要而又系統性的呈現深度學習所涉及到的基本模型和相關概念。論文&ldquo;<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.00019">A Critical Review of Recurrent Neural Networks for Sequence Learning</a>&rdquo;中的示意圖畫得簡單而又形象，足以說明問題，但這篇文章僅就 RNN 而展開論述，並未涉及 CNN，RBM 等其它經典模型；<a href="http://link.zhihu.com/?target=https%3A//deeplearning4j.org/cn/neuralnet-overview.html%23forward">Deeplearning4j</a>上的教程貌似缺少關於編碼器相關內容的介紹，而<a href="http://link.zhihu.com/?target=http%3A//ufldl.stanford.edu/wiki/index.php/UFLDL%25E6%2595%2599%25E7%25A8%258B">UFLDL 教程</a>只是詳細介紹了編碼器的方方面面。但是如果照抄以上三篇的圖例，又涉及到圖例中的模塊和符號不統一的問題。所以，索性自己畫了部分模型圖；至於直接引用的圖，文中已經給了鏈接或出處。如有不妥之處，望指正。以下，以饗來訪。</p>
<p><strong>1. </strong> 從經典的二分類開始說起，爲此構建二分類的神經網絡單元，並以 Sigmoid 函數和平方差損失（比較常用的還有交叉熵損失函數）函數來舉例說明<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA5ODUxOTA5Mg%3D%3D%26mid%3D2652550294%26idx%3D1%26sn%3D820ddc89e1d1af35f14ccf645b963a76%26chksm%3D8b7e45cdbc09ccdb985b3bbc22fbc0dcd013d53e9e9a6073d09d1b676b338af7bd8b7dd2a92d%26mpshare%3D1%26scene%3D1%26srcid%3D0930LxixeTcq5wCcRStBTylE%26pass_ticket%3Dw2yCF%252F3Z2KTqyWW%252FUwkvnidRV3HF9ym5iEfJ%252BZ1dMObpcYUW3hQymA4BpY9W3gn4%23rd">梯度下降法</a>以及基於鏈式法則的反向傳播（BP），所有涉及到的公式都在這裏：</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-eabfde057dc289980a47b30cb4f13d21_b.jpg" alt="">
<p><strong>2. </strong>神經元中的非線性變換激活函數（<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI1NTE4NTUwOQ%3D%3D%26mid%3D2650325236%26idx%3D1%26sn%3D7bd8510d59ddc14e5d4036f2acaeaf8d%26mpshare%3D1%26scene%3D1%26srcid%3D1214qIBJrRhevScKXQQuqas4%26pass_ticket%3Dw2yCF%252F3Z2KTqyWW%252FUwkvnidRV3HF9ym5iEfJ%252BZ1dMObpcYUW3hQymA4BpY9W3gn4%23rd">深度學習中的激活函數導引</a>）及其作用（參考<a href="https://www.zhihu.com/people/yan-qin-rui">顏沁睿</a>的<a href="https://www.zhihu.com/question/22334626/answer/103835591">回答</a>），激活函數是神經網絡強大的基礎，好的激活函數（根據任務來選擇）還可以加速訓練：</p>
<img class="content-image" src="http://pic1.zhimg.com/70/v2-9227fb8304a64498209ea07772cdd7e0_b.jpg" alt="">
<p><strong>3.</strong> 前饋性神經網絡和自動編碼器的區別在於輸出層，從而引出無監督學習的概念；而降噪編碼器和自動編碼器的區別又在輸入層，即對輸入進行部分遮擋或加入噪聲；稀疏編碼器（引出<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA5ODUxOTA5Mg%3D%3D%26mid%3D2652549828%26idx%3D1%26sn%3D5b77192a91d593342e00a984f1132c50%26mpshare%3D1%26scene%3D1%26srcid%3D0802dqmt7jHAo8DZuBVYHO7T%26pass_ticket%3Dw2yCF%252F3Z2KTqyWW%252FUwkvnidRV3HF9ym5iEfJ%252BZ1dMObpcYUW3hQymA4BpY9W3gn4%23rd">正則項</a>的概念）和自動編碼器的區別在隱藏層，即隱藏層的節點數大於輸入層節點數；而編碼器都屬於無監督學習的範疇。淺層網絡的不斷棧式疊加構成相應的深度網絡。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-d8190365ba77b450d64de91f4538e2a9_b.jpg" alt="">
<p>值得一提的是，三層前饋型神經網絡（只包含一個隱藏層）的 word2vec(<a href="http://link.zhihu.com/?target=http%3A//suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178/1">數學原理詳解</a>)是邁向 NLP 的大門，包括 CBOW 和 skip-gram 兩種模型，另外在輸出層還分別做了基於 Huffman 樹的 Hierarchical Softmax 以及 negative sampling（就是選擇性地更新連接負樣本的權重參數）的加速。</p>
<p><strong>4. </strong>受限波茲曼機 RBM 屬於無監督學習中的生成學習，輸入層和隱藏層的傳播是雙向的，分正向過程和反向過程，學習的是數據分佈，因此又引出馬爾可夫過程和 Gibbs 採樣的概念，以及 KL 散度的度量概念：</p>
<img class="content-image" src="http://pic4.zhimg.com/70/v2-cd53ee50bf533b453408318d3f13dc73_b.jpg" alt="">
<p>與生成學習對應的是判別學習也就是大多數的分類器，生成對抗網絡 GAN 融合兩者；對抗是指生成模型與判別模型的零和博弈，近兩年最激動人心的應用是從文本生成圖像（<a href="http://link.zhihu.com/?target=http%3A//www.evolvingai.org/ppgn">Evolving AI Lab - University of Wyoming</a>）：</p>
<img class="content-image" src="http://pic4.zhimg.com/70/v2-53b180b2f2b7e9f37f712c8ae12e8f43_b.jpg" alt="">
<p><strong>5. </strong>深度網絡的實現基於逐層貪心訓練算法，而隨着模型的深度逐漸增加，會產生梯度消失或梯度爆炸的問題，梯度爆炸一般採用閾值截斷的方法解決，而梯度消失不易解決；網絡越深，這些問題越嚴重，這也是深度學習的核心問題，出現一系列技術及衍生模型。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-247ecfca25fcad04aa4122eb1e892765_b.jpg" alt="">
<p>深度制勝，網絡越深越好，因此有了<a href="http://link.zhihu.com/?target=http%3A//xueshu.baidu.com/s%3Fwd%3Dpaperuri%253A%25283821a90f58762386e257eb4e6fa11f79%2529%26filter%3Dsc_long_sign%26tn%3DSE_xueshusource_2kduw22v%26sc_vurl%3Dhttp%253A%252F%252Farxiv.org%252Fabs%252F1512.03385%26ie%3Dutf-8%26sc_us%3D13213678896270879240">深度殘差網絡</a>將深度擴展到 152 層，並在 ImageNe 多項競賽任務中獨孤求敗：</p>
<img class="content-image" src="http://pic3.zhimg.com/70/7b3ee9e4f4a2e61acf35820a2768cc12_b.jpg" alt="">
<p><strong>6.</strong> 卷積神經網絡在層與層之間採取局部鏈接的方式，即卷積層和採樣層，在計算機視覺的相關任務上有突出表現，關於卷積神經網絡的更多介紹請參考我的另一篇文章（<a href="https://zhuanlan.zhihu.com/p/21699462">戳戳戳</a>）：</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-aa4855dc4cf11dc0bff5c131a278c4e9_b.jpg" alt="">
<p>而在 NIPS 2016 上來自康奈爾大學計算機系的副教授 Killan Weinberger 探討了深度極深的卷積網絡，在數據集 CIFAR-10 上訓練一個 <a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.09382v3.pdf">1202 層深的網絡</a>。</p>
<p><strong>7.</strong> 循環神經網絡在隱藏層之間建立了鏈接，以利用時間維度上的歷史信息和未來信息，與此同時在時間軸上也會產生梯度消失和梯度爆炸現象，而 LSTM 和 GRU 則在一定程度上解決了這個問題，兩者與經典 RNN 的區別在隱藏層的神經元內部結構，在語音識別，NLP（比如 RNNLM）和機器翻譯上有突出表現（<a href="http://link.zhihu.com/?target=http%3A//karpathy.github.io/2015/05/21/rnn-effectiveness/">推薦閱讀</a>）：</p>
<img class="content-image" src="http://pic1.zhimg.com/70/v2-057c850ac4e3275ab87440af8b446ac8_b.jpg" alt="">
<p>RNN 模型在一定程度上也算是分類器，在圖像描述（<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/deepimagesent/">Deep Visual-Semantic Alignments for Generating Image Descriptions</a>）的任務中已經取得了不起的成果（第四節 GAN 用文本生成圖像是逆過程，注意區別）：</p>
<img class="content-image" src="http://pic4.zhimg.com/70/v2-efb1bc6baa74b79ab5ff2757e75f3337_b.jpg" alt="">
<p>另外，關於 RNN 的最新研究是基於 attention 機制來建立模型（<a href="http://link.zhihu.com/?target=http%3A//distill.pub/2016/augmented-rnns/">推薦閱讀文章</a>），即能夠在時間軸上選擇有效信息加以利用，比如百度 App 中的"<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.09889">爲你寫詩</a>"的功能核心模型就是 attention-based RNN encoder-decoder：</p>
<img class="content-image" src="http://pic3.zhimg.com/70/v2-3463b8dbd7ed0e92c21d5f73043513fe_b.jpg" alt="">
<img class="content-image" src="http://pic3.zhimg.com/70/v2-205b838a0cb13544f14f25e28d9bb8b6_b.jpg" alt="">
<p><strong>8.</strong> 總結了深度學習中的基本模型並再次解釋部分相關的技術概念：</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-487f13a5de2ef2105a90be878e5f3ed5_b.jpg" alt="">
<p>最後，現在深度學習在工業中的應用往往是整合多個模型到產品中去，比如在語音識別的端到端系統中，利用無監督模型或者 CNN 作爲前期處理提取特徵，然後用 RNN 模型進行邏輯推理和判斷，從而達到可媲美人類交流的水平，如百度的<a href="http://link.zhihu.com/?target=http%3A//xueshu.baidu.com/s%3Fwd%3Dpaperuri%253A%25281ba47fa102a2d61cb4a8a5d85049707c%2529%26filter%3Dsc_long_sign%26tn%3DSE_xueshusource_2kduw22v%26sc_vurl%3Dhttp%253A%252F%252Farxiv.org%252Fabs%252F1512.02595%26ie%3Dutf-8%26sc_us%3D8168572815923394227">DeepSpeech2</a>:</p>
<img class="content-image" src="http://pic4.zhimg.com/70/v2-79855f5d20b86111df9c374863815e5f_b.jpg" alt="">
<p>畫圖是個細活慢活，週末加班很辛苦，覺得好就動動手指給個贊吧。</p>
<p>不喜請噴，轉載請註明出處，謝謝。</p>
</div>
</div>




</div>


</div>
</div>