+++
date = "2016-10-25T11:00:00"
title = "賭場抓老千只是鬧着玩兒，語音識別纔是它的真本事"
titleimage = "http://pic1.zhimg.com/de7bb8b94d9fa351577865b00963d2b8.jpg"
ga = 102511
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">如何用簡單易懂的例子解釋隱馬爾可夫模型？</h2>

<div class="answer">



<div class="content">
<p><strong>1. 賭場風雲（背景介紹）<br></strong></p>
<img class="content-image" src="http://pic2.zhimg.com/70/240ab89afad8bff33e442a8d95433831_b.jpg" alt="">
<p>最近一個賭場的老闆發現生意不暢，於是派出手下去賭場張望。經探子回報，有位大叔在賭場中總能贏到錢，玩得一手好<strong>骰子</strong>，幾乎是戰無不勝。而且每次玩骰子的時候周圍都有幾個保鏢站在身邊，讓人不明就裏，只能看到每次開局，骰子飛出，沉穩落地。老闆根據多年的經驗，推測這位不善之客使用的正是江湖失傳多年的"偷換骰子大法&rdquo;（編者注:<strong>偷換骰子大法，用兜裏自帶的骰子偷偷換掉均勻的骰子</strong>）。老闆是個冷靜的人，看這位大叔也不是善者，不想輕易得罪他，又不想讓他壞了規矩。正愁上心頭，這時候進來一位名叫 HMM 帥哥，告訴老闆他有一個很好的解決方案。</p>
<p>不用近其身，只要在遠處裝個攝像頭，把每局的骰子的點數都記錄下來。</p>
<p>然後 HMM 帥哥將會運用其強大的數學內力，用這些數據推導出</p>
<p>1. 該大叔是不是在出千?</p>
<p>2. 如果是在出千，那麼他用了幾個作弊的骰子?　還有當前是不是在用作弊的骰子。</p>
<p>3. 這幾個作弊骰子出現各點的概率是多少?</p>
<p>天吶，老闆一聽，這位叫 HMM 的甚至都不用近身，就能算出是不是在作弊，甚至都能算出別人作弊的骰子是什麼樣的。那麼，只要再當他作弊時，派人圍捕他，當場驗證骰子就能讓他啞口無言。</p>
<p><strong>2. HMM 是何許人也?</strong></p>
<p>在讓 HMM 開展調查活動之前，該賭場老闆也對 HMM 作了一番調查。</p>
<p>HMM（Hidden Markov Model）, 也稱隱性馬爾可夫模型，是一個概率模型，用來描述一個系統<strong>隱性狀態</strong>的轉移和<strong>隱性狀態</strong>的表現概率。</p>
<p><strong>系統的隱性狀態</strong>指的就是一些外界不便觀察（或觀察不到）的狀態, 比如在當前的例子裏面, 系統的狀態指的是大叔使用骰子的狀態，即</p>
<p>{正常骰子, 作弊骰子 1, 作弊骰子 2,...}</p>
<p><strong>隱性狀態的表現</strong>也就是, 可以觀察到的，由隱性狀態產生的外在表現特點。這裏就是說, 骰子擲出的點數.</p>
<p>{1,2,3,4,5,6}</p>
<p>HMM 模型將會描述，系統<strong>隱性狀態的轉移概率</strong>。也就是大叔切換骰子的概率,下圖是一個例子，這時候大叔切換骰子的可能性被描述得淋漓盡致。</p>
<img class="content-image" src="http://pic3.zhimg.com/70/de1e09aa9c09b1a0928f5b91ba45d352_b.jpg" alt="">
<p>很幸運的，這麼複雜的概率轉移圖，竟然能用簡單的矩陣表達, 其中 a_{ij}代表的是從 i 狀態到 j 狀態發生的概率</p>
<img class="content-image" src="http://pic1.zhimg.com/70/0a28bf2d267ba6aa16dde74771796bbc_b.jpg" alt="">
<p>當然同時也會有，<strong>隱性狀態表現轉移</strong>概率。也就是骰子出現各點的概率分佈, （e.g. 作弊骰子 1 能有 90% 的機會擲到六，作弊骰子 2 有 85% 的機會擲到'小&rsquo;）. 給個圖如下,</p>
<img class="content-image" src="http://pic2.zhimg.com/70/2f6e49ac23b9f99cc21cf9016435ae39_b.jpg" alt="">
<p>隱性狀態的表現分佈概率也可以用矩陣表示出來，</p>
<img class="content-image" src="http://pic3.zhimg.com/70/9974153a9ac8963c329c4195878c5312_b.jpg" alt="">
<p>把這兩個東西總結起來，就是整個 HMM 模型。</p>
<p>這個模型描述了隱性狀態的轉換的概率，同時也描述了每個狀態外在表現的概率的分佈。總之，HMM 模型就能夠描述扔骰子大叔作弊的頻率（骰子更換的概率），和大叔用的骰子的概率分佈。有了大叔的 HMM 模型，就能把大叔看透，讓他完全在陽光下現形。</p>
<p><strong>3. HMM 能幹什麼!</strong></p>
<p>總結起來 HMM 能處理三個問題，</p>
<p>3.1 解碼（Decoding）</p>
<p>解碼就是需要從一連串的骰子中，看出來哪一些骰子是用了作弊的骰子，哪些是用的正常的骰子。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/4a40f1a93aa25adce92f5441e6e2cfb9_b.jpg" alt="">
<p>比如上圖中，給出一串骰子序列（3,6,1,2..）和大叔的 HMM 模型, 我們想要計算哪一些骰子的結果（隱性狀態表現）可能對是哪種骰子的結果（隱性狀態）.</p>
<p>3.2 學習（Learning）</p>
<p>學習就是，從一連串的骰子中，學習到大叔切換骰子的概率，當然也有這些骰子的點數的分佈概率。這是 HMM 最爲<strong>恐怖</strong>也最爲<strong>複雜</strong>的招數！！</p>
<p>3.3 估計（Evaluation）</p>
<p>估計說的是，在我們<strong><em>已經知道</em></strong>了該大叔的 HMM 模型的情況下，估測某串骰子出現的可能性概率。比如說，在我們已經知道大叔的 HMM 模型的情況下，我們就能直接估測到大叔扔到 10 個 6 或者 8 個 1 的概率。</p>
<p><strong>4. HMM 是怎麼做到的?</strong></p>
<p>4.1 估計</p>
<p>估計是最容易的一招，在完全知道了大叔的 HMM 模型的情況下，我們很容易就能對其做出估計。</p>
<p>現在我們有了大叔的狀態轉移概率矩陣 A,B 就能夠進行估計。比如我們想知道這位大叔下一局連續擲出 10 個 6 的概率是多少? 如下</p>
<img class="content-image" src="http://pic3.zhimg.com/70/f893ed100b894616bffef0bae9f461da_b.jpg" alt="">
<p>這表示的是，在一開始隱性狀態（s0）爲 1，也就是一開始拿着的是<strong>正常的骰子</strong>的情況下，這位大叔連續擲出 10 個 6 的概率。</p>
<p>現在問題難就難在，我們雖然知道了 HMM 的轉換概率，和觀察到的狀態 V{1:T}, 但是我們卻不知道實際的隱性的狀態變化。</p>
<p>好吧，我們不知道隱性狀態的變化，那好吧，我們就先<strong>假設</strong>一個<strong>隱性狀態序列</strong>, 假設大叔前 5 個用的是正常骰子, 後 5 個用的是作弊骰子 1.</p>
<img class="content-image" src="http://pic3.zhimg.com/70/468da543a29861571622d107d845bc42_b.jpg" alt="">
<p>好了，那麼我們可以計算，在這種隱性序列假設下擲出 10 個 6 的概率.</p>
<img class="content-image" src="http://pic1.zhimg.com/70/4f15b94f31eb9fbe8370a1ad6b90e730_b.jpg" alt="">
<p>這個概率其實就是，隱性狀態表現概率 B 的乘積.</p>
<img class="content-image" src="http://pic4.zhimg.com/70/c1616bab1b90e3daa3b01c93a27f5c07_b.jpg" alt="">
<p>但是問題又出現了，剛纔那個隱性狀態序列是我假設的，而實際的序列我不知道，這該怎麼辦。好辦，把<strong>所有可能出現</strong>的隱狀態序列組合全都試一遍就可以了。於是,</p>
<img class="content-image" src="http://pic3.zhimg.com/70/b007c2e9628c6db1de871bc5c52e24b6_b.jpg" alt="">
<p>R 就是所有可能的隱性狀態序列的集合。的嗯，現在問題好像解決了，我們已經能夠通過嘗試所有組合來獲得出現的概率值，並且可以通過 A,B 矩陣來計算出現的總概率。</p>
<p>但是問題又出現了，可能的集合太大了, 比如有三種骰子，有 10 次選擇機會, 那麼總共的組合會有 3^10 次...這個量級 O（c^T）太大了，當問題再大一點時候，組合的數目就會大得超出了計算的可能。所以我們需要一種更有效的計算 P（V（1:T）概率的方法。</p>
<p>比如說如下圖的算法可以將計算 P（V1:T）的計算複雜度降低至 O（cT）.</p>
<img class="content-image" src="http://pic4.zhimg.com/70/7a7efeb3e50a827cd8305f1c1dde360f_b.jpg" alt="">
<p>有了這個方程，我們就能從 t=0 的情況往前推導，一直推導出 P（V1:T）的概率。下面讓我們算一算，大叔擲出 3,2,1 這個骰子序列的可能性有多大（假設初始狀態爲 1, 也就是大叔前一次拿着的是正常的骰子）?</p>
<p><strong>4.2 解碼（Decoding）</strong></p>
<p>解碼的過程就是在給出一串序列的情況下和已知 HMM 模型的情況下，找到最可能的隱性狀態序列。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/4a40f1a93aa25adce92f5441e6e2cfb9_b.jpg" alt="">
<p>用數學公式表示就是, （V 是 Visible 可見序列, w 是隱性狀態序列, A,B 是 HMM 狀態轉移概率矩陣）</p>
<p>（公式太多，請具體看我博客中的推導<a href="http://blog.csdn.net/ppn029012/article/details/8923501">機器學習 --- 4. 大內密探 HMM（隱馬爾可夫）圍捕賭場老千</a>）</p>
<p>然後又可以使用<strong>估計（4.1）</strong>中的<strong>前向推導法</strong>，計算出最大的 P（w（1:T）, V（1:T））.</p>
<p>在完成前向推導法之後，再使用<strong>後向追蹤法</strong>（Back Tracking），對求解出能令這個 P（w（1:T）, V（1:T））最大的隱性序列.這個算法被稱爲維特比算法（Viterbi Algorithm）.</p>
<p><strong>4.3 學習（Learning）</strong></p>
<p>學習是在給出 HMM 的結構的情況下（比如說假設已經知道該大叔有 3 只骰子，每隻骰子有 6 面）,計算出最有可能的模型參數.</p>
<p>（公式太多，請具體看我博客中的推導<a href="http://blog.csdn.net/ppn029012/article/details/8923501">機器學習 --- 4. 大內密探 HMM（隱馬爾可夫）圍捕賭場老千</a>）</p>
<p><strong>5. HMM 的應用</strong></p>
<p>以上舉的例子是用 HMM 對擲骰子進行建模與分析。當然還有很多 HMM 經典的應用，能根據不同的應用需求，對問題進行建模。</p>
<p>但是使用 HMM 進行建模的問題，必須滿足以下條件,</p>
<p>1.<strong>隱性狀態的轉移</strong>必須滿足馬爾可夫性。（狀態轉移的馬爾可夫性:一個狀態只與前一個狀態有關）</p>
<p>2. 隱性狀態必須能夠大概被估計。</p>
<p>在滿足條件的情況下,確定問題中的<strong>隱性狀態是什麼</strong>,隱性狀態的<strong>表現</strong>可能又有哪些.</p>
<p>HMM 適用於的問題在於，真正的狀態（隱態）難以被估計，而狀態與狀態之間又存在聯繫。</p>
<p><strong>5.1 語音識別</strong></p>
<p>語音識別問題就是將一段<strong>語音信號</strong>轉換爲<strong>文字序列</strong>的過程. 在個問題裏面</p>
<p>隱性狀態就是: 語音信號對應的文字序列</p>
<p>而顯性的狀態就是: 語音信號.</p>
<img class="content-image" src="http://pic1.zhimg.com/70/8ea5a05ed423c75e283dac52bc378604_b.jpg" alt="">
<p>HMM 模型的學習（Learning）: 語音識別的模型學習和上文中通過觀察骰子序列建立起一個最有可能的模型<strong>不同</strong>.　語音識別的 HMM 模型學習有兩個步驟:</p>
<p>1. 統計文字的發音概率,建立隱性表現概率矩陣Ｂ</p>
<p>2. 統計字詞之間的轉換概率（這個步驟並不需要考慮到語音,可以直接統計字詞之間的轉移概率即可）</p>
<p>語音模型的估計（Evaluation）: 計算"是十四&rdquo;,"四十四"等等的概率,比較得出最有可能出現的文字序列.</p>
<p><strong>5.2 手寫識別</strong></p>
<p>這是一個和語音差不多,只不過手寫識別的過程是將字的圖像當成了顯性序列.</p>
<p><strong>5.3 中文分詞</strong></p>
<p>&ldquo;總所周知，在漢語中，詞與詞之間不存在分隔符（英文中，詞與詞之間用空格分隔，這是天然的分詞標記），詞本身也缺乏明顯的形態標記，因此，中文信息處理的特有問題就是如何將漢語的字串分割爲合理的詞語序。例如，英文句子：you should go to kindergarten now 天然的空格已然將詞分好，只需要去除其中的介詞&ldquo;to&rdquo;即可；而&ldquo;你現在應該去幼兒園了&rdquo;這句表達同樣意思的話沒有明顯的分隔符，中文分詞的目的是，得到&ldquo;你 / 現在 / 應該 / 去 / 幼兒園 / 了&rdquo;。那麼如何進行分詞呢？主流的方法有三種：第 1 類是基於語言學知識的規則方法，如：各種形態的最大匹配、最少切分方法；第 2 類是基於大規模語料庫的機器學習方法，這是目前應用比較廣泛、效果較好的解決方案．用到的統計模型有 N 元語言模型、信道&mdash;噪聲模型、最大期望、HMM 等。第 3 類也是實際的分詞系統中用到的，即規則與統計等多類方法的綜合。&rdquo;[1]<a href="http://www.52nlp.cn/itenyh%E7%89%88-%E7%94%A8hmm%E5%81%9A%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%B8%80%EF%BC%9A%E5%BA%8F">使用 HMM 進行中文分詞.</a></p>
<p>5.4 HMM 實現拼音輸入法</p>
<p>拼音輸入法,是一個估測拼音字母對應想要輸入的文字（隱性狀態）的過程（比如, &lsquo;pingyin&rsquo; -&gt; 拼音）</p>
<p><a href="http://www.sobuhu.com/archives/1008">使用 HMM 實現簡單拼音輸入法</a></p>
<p>參考:</p>
<p><a href="http://ai.stanford.edu/~serafim/CS262_2007/notes/lecture5.pdf">http://ai.stanford.edu/~serafim/CS262_2007/notes/lecture5.pdf</a></p>
</div>
</div>




</div>


</div>
</div>