+++
date = "2016-10-18T07:00:00"
title = "個性化推薦越來越多，會因此錯過不同意見嗎？"
titleimage = "http://pic1.zhimg.com/d60eec5f684b96651ae918575a9ceb94.jpg"
ga = 101807
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">社交網絡爲用戶進行「個性化推薦」的做法，是否會導致人們「難以接觸到意見相左的人」？</h2>

<div class="answer">



<div class="content">
<p>Facebook 幾位數據科學家 2015 年在 <em>Science</em> 上發文，<a href="http://science.sciencemag.org/content/early/2015/05/06/science.aaa1160.full">表示&ldquo;用戶對信息的主動篩選是導致他們接收信息單一的主要原因，網站的新鮮事算法排序對此影響較小&rdquo;</a>。文章遭到學術界的尖銳批評，被某些學者稱爲&ldquo;洗白&rdquo;論文。爭議集中在該研究的樣本和結論兩方面。</p>
<p>目前我們對&ldquo;算法排序是否阻礙多元信息傳播&rdquo;這個問題還沒有統一答案。但社交媒體普及不過才幾年時間，日後應該會有更多權威研究。</p>
<p>--</p>
<p>先來看 Facebook 的這項研究。研究分析了 2014/07/07 到 2015/01/07 半年來用戶在個人主頁上看到及點開的新聞鏈接。研究人員計算了每位用戶和每個新聞鏈接的&ldquo;政治傾向分數&rdquo;，由此判定哪些內容和用戶意見相左。（政治在這裏是一個非常廣泛的概念。總統競選是政治，墮胎、轉基因也是政治。）</p>
<p>文章的核心發現就是下面這幅圖：</p>
<img class="content-image" src="http://pic1.zhimg.com/70/985922809a3d6b27b7458343e520fcc8_b.jpg" alt="">
<p>可以看出，如果 Facebook 隨機向用戶展示全網正在被分享的內容（忽略好友關係），自由和保守派看到的內容有超過 40% 都來自對方陣營。好友關係（第二列）大幅減少了這個比例，讓信息變得更單一，因爲保守派的朋友多是保守派，自由派的朋友多是自由派。</p>
<p><strong>再看右邊兩列：算法排序篩掉了 5% 與保守派意見相左的內容，8% 與自由派意見相左的內容。而用戶有選擇性的點擊進一步篩掉了與他們意見相左的內容，分別是保守派 17%，自由派 6%。</strong></p>
<p>由此，研究人員得出如下結論：</p>
<blockquote>我們毫無疑問地證明，在限制 Facebook 多元信息傳播這個問題上，個人選擇比算法的影響更大。</blockquote>
<p>--</p>
<p>文章發表後遭到學術界的尖銳批評。主要問題如下：</p>
<p>1. 該研究的樣本不是隨機選取的，而是限定在&ldquo;一週登錄 4 - 7 次&rdquo;且&ldquo;在 Facebook 上標註了自己政治傾向&rdquo;的用戶。<a href="http://science.sciencemag.org/content/sci/suppl/2015/05/06/science.aaa1160.DC1/Bakshy-SM.revision.1.pdf">雖然樣本數量很大</a>，但並不具代表性，樣本選取極有可能對結果有影響。比如，Facebook 絕大多數用戶（91%）沒有標明政治傾向，對政治可能沒那麼關心，那麼算法排序對他們的影響可能會更大。論文不該用&ldquo;毫無疑問地證明 Facebook 上...&rdquo;這種話來誤導讀者，讓大家以爲論文的結論適用於所有 Facebook 用戶。</p>
<p><a href="https://solomonmessing.wordpress.com/2015/05/24/exposure-to-ideologically-diverse-news-and-opinion-future-research/">論文作者對此迴應說</a>，標註了政治傾向的用戶一般更積極參與政治，所以研究這些人也是很有價值的。以後當然也可以研究其他羣體。</p>
<p>2. 不能因爲&ldquo;算法&rdquo;的影響比&ldquo;個人選擇&rdquo;的影響小，就給算法&ldquo;洗白&rdquo;。學術界很早就發現&ldquo;人們更可能選擇瀏覽和自己意見相似的內容&rdquo;，這點沒有爭議。大家想知道的只是算法排序有沒有加劇這一現象。</p>
<p>密歇根大學一學者批評說，這篇論文就好比菸草公司選擇對煤炭工人進行研究，<a href="https://socialmediacollective.org/2015/05/07/the-facebook-its-not-our-fault-study/#notes">發現吸菸給人帶來的危害不如比挖炭</a>。正確的結論應該是&ldquo;吸菸和挖炭都對人有害&rdquo;，就好比 Facebook 這篇論文的結論應該是&ldquo;算法和人工選擇都限制了用戶接觸與他們意見相左的內容&rdquo;。&ldquo;算法排序&rdquo;和&ldquo;人工選擇&rdquo;在這裏是 and（和），不是 or（或）。</p>
<p>3. 就算我們來比較&ldquo;算法&rdquo;和&ldquo;個人選擇&rdquo;哪個影響更大，論文的結論也只在保守派這裏成立。算法篩掉了 8% 與自由派用戶意見相左的內容，而他們主動選擇只刪掉了 6%。</p>
<p>4. 因爲只有 Facebook 內部有數據，所以其他人無法重複這個研究。如果另一組學者按照同樣的方法研究，會得到相同結論嗎？Facebook 的算法每天都在變，我們可以確定今年的算法和去年的算法對信息的篩選效果一樣嗎？</p>
<p>--</p>
<p>其實解決這些問題最簡單的辦法就是 Facebook 自己做一個實驗，實驗組的新鮮事頁面經過算法排序，對照組的沒有算法參與，完全逆序排列。但 Facebook 前幾年因爲在用戶身上做實驗<a href="http://www.theatlantic.com/technology/archive/2014/09/facebooks-mood-manipulation-experiment-might-be-illegal/380717/">已經遭到社會批評且涉嫌違法</a>，所以再次大規模做實驗估計不太可能。更重要的是，Facebook 作爲商業公司，對算法排序是否加劇大衆意見兩極化也不太感興趣。</p>
<p>希望以後學者可以設計厲害的實驗來回答這個問題。不管是觀察還是做實驗，都要牽扯到數據科學、計算機、計量、心理學 / 社會學 / 傳播學 / 政治學理論，確實是有意思的跨學科問題。</p>
<p>--</p>
<p>相關研究：</p>
<ul>
<li><a href="http://social.cs.uiuc.edu/papers/pdfs/Eslami_Algorithms_CHI15.pdf">超過半數用戶都不知道他們在 Facebook 看見的內容是經過算法排序的</a>；當他們看不見好友發的東西時，很多人以爲是好友故意不讓他們看</li>
<li>我們在 Facebook 上互動最多的內容是&ldquo;強關係&rdquo;好友分享的，<a href="https://www.facebook.com/notes/facebook-data-science/rethinking-information-diversity-in-networks/10150503499618859/">但我們看到的新鮮內容大都由&ldquo;弱關係&rdquo;分享</a></li>
<li>大衆媒體是否加劇民衆意見兩極化？答案也是&ldquo;我們無法確定&rdquo;（見 Prior, Gentzkow, Iyengar）</li>
</ul>
<hr>
<p>Ask me anything: <a href="https://www.zhihu.com/zhi/people/723589957574234112">Yiqin Fu 的值乎 - 說點兒有用的</a></p>
</div>
</div>




</div>


</div>
</div>