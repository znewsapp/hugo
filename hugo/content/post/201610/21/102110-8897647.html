+++
date = "2016-10-21T10:00:00"
title = "大腦是怎麼工作的？快讓機器跟着學學"
titleimage = "http://pic2.zhimg.com/fd62a948774de70312bbb989f7607e11.jpg"
ga = 102110
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">



<div class="question">
<h2 class="question-title">機器學習和神經科學：你的大腦也在進行深度學習嗎？</h2>
<div class="answer">



<div class="content">
<p><strong>導語</strong></p>
<p>深度學習的靈感起源可能是神經科學，但近年來的發展毫無疑問已經自成一派，（幾乎）與神經科學無關了。機器學習專家們感興趣的是如何進一步優化他們的算法；神經科學家們則更想知道人腦，而非深度網絡們，是如何工作的。</p>
<img class="content-image" src="https://pic4.zhimg.com/v2-4836db6f8e449a3c5fcc7f539b4173e7_b.jpg" alt="">
<p><em>這一 &ldquo;大腦電路&rdquo; 圖像同時被計算機學家們和生物學家們兩方面所擯棄&mdash;&mdash;它既不是一副真正的深度網絡結構圖，也不能描繪大腦的工作原理。</em></p>
<p>Konrad Kording 試圖改變這一趨勢，<strong>重啓神經科學和機器學習之間的對話</strong>。他與 Adam Marblestone (MIT Media Lab) 以及 Greg Wayne (Google Deepmind) 合作的文章《走向深度學習與神經科學的結合》闡述了這一理念，6 月存檔於 bioRxiv，9 月發表於《計算神經科學前沿》。</p>
<p>一些讀者可能看過 <a class="internal" href="https://zhuanlan.zhihu.com/p/21320667">神經科學家能理解微處理器嗎？大數據時代神經科學的理論困境</a>，也是介紹 Kording 的作品。如果說上一篇文章提出了一個尖銳的問題&mdash;&mdash;神經科學現有研究手段是否令人滿意&mdash;&mdash;這篇文章或許可以被看作是解決問題的途徑之一：採納深度學習中發展出的思想來研究大腦。文章很長，涉及的內容較多，在這裏先只介紹總體思路，許多分支儘管有趣會暫時略過。十分推薦<a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//journal.frontiersin.org/article/10.3389/fncom.2016.00094/full" target="_blank" rel="nofollow noreferrer">閱讀原文</a>（開放訪問）。</p>
<p><strong>現代機器學習的三個趨勢</strong></p>
<p>作者首先指出現代機器學習的三個特徵：</p>
<ul>
<li>專注於優化成本函數</li>
<li>近期工作引入複雜的成本函數</li>
<ul>
<li>這包括空間和時間上不平均的函數，和網絡內部產生的成本函數。</li>
</ul>
<li>機器學習的結構本身也變得越來越多元化</li>
<ul>
<li>新發展的結構包括記憶單元，&ldquo;膠囊&rdquo;，外部記憶，指針和硬編碼的算術指令等。</li>
</ul>
</ul>
<p><strong>大腦工作方式的三個假說</strong></p>
<p>指出上面的三個機器學習特徵後，作者提出了三個假說：</p>
<ul>
<li><em>假說一：大腦優化成本函數 The Brain Optimizes Cost Functions<br></em></li>
<li><em>假說二：不同腦區在發展的不同時期使用多樣化的成本函數 Cost Functions Are Diverse across Areas and Change over Development</em></li>
<li><em>假說三：大腦中的專門系統高效解決關鍵計算問題 Specialized System Allow Efficient Solution of Key Computational Problems</em></li>
</ul>
<p><strong>跑題一分鐘，此小節可以跳過</strong></p>
<p>老實說，我 6 月份看到這裏的感覺是&mdash;&mdash;</p>
<p>好坑爹！</p>
<p>看上去這像是又一篇空洞而無解釋力的腦洞文章：成本函數是一個特別寬泛的概念，爲神經系統的活動找到成本函數很平凡。假說二明顯是個補丁嘛！找不到全局成本函數就說是局部的，又怕時間上不穩定就說是發展的結果。。。假說三什麼都沒說啊，腦區專業誰不知道？</p>
<p>就這樣，我把文章丟到一邊。隔了三個月文章居然過了評審，才又下了新版本來看。</p>
<p>這次我先跳到最後看了結論，結果一下子就被吸引住了：</p>
<blockquote>In other words, this framework could be viewed as proposing a kind of &ldquo;society&rdquo; of cost functions and trainable networks, permitting internal bootstrapping processes reminiscent of the Society of Mind (<a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//journal.frontiersin.org/article/10.3389/fncom.2016.00094/full%23B305" target="_blank" rel="nofollow noreferrer">Minsky, 1988</a>). In this view, intelligence is enabled by many computationally specialized structures, each trained with its own developmentally regulated cost function, where both the structures and the cost functions are themselves optimized by evolution like the hyperparameters in neural networks.<br>換句話說，這一框架可以被看成是一種由成本函數和可訓練網絡所構成的&ldquo;社會&rdquo;，從而實現類似於閔斯基在《心靈的社會》中提到的內在自舉過程。在這一觀點中，智能是由許多特別的計算結構所實現的，每一個是由其受控於發展的成本函數所訓練，而結構本身和成本函數都像超參數一樣由進化所優化 。</blockquote>
<p>雖然只不過是換了句話來說，不知怎麼就覺得很符合直覺了。也許是閔斯基的大名加持，不過更可能的是成本函數用生物學家的話來說就是驅動力 (drive)，而驅動力的多元化是我最近恰好在思考的一個問題。</p>
<p><strong>跑題結束，歡迎回來</strong></p>
<img class="content-image" src="https://pic3.zhimg.com/v2-d5f273316b98787c60dde15d2a173fa6_b.jpg" alt="">
<p>上面是本篇嘴炮文章的唯一一副示意圖。圖 A 是傳統機器學習的典型結構，紅色虛線爲成本函數（以誤差的形式輸入網絡）。圖 B 則是假想的大腦神經網絡，成本函數是根據外部輸入在系統內部計算而得出的。圖 C 中，多個不同腦區分別根據不同的成本函數訓練，並彼此互相影響。</p>
<p><strong>大腦可以優化成本函數 The brain can optimize cost functions</strong></p>
<p>一個試圖統一機器學習與神經系統的理論所遇到的第一個詰難一定是：神經系統怎麼可能實現反向傳播 (backpropagation)？？？</p>
<p>這是一個首要的問題，作者一口氣寫了八頁紙。其核心思想爲</p>
<blockquote>(a) the brain has powerful mechanisms for credit assignment during learning that allow it to optimize global functions in multi-layer networks by adjusting the properties of each neuron to contribute to the global outcome, and that (b) the brain has mechanisms to specify exactly which cost functions it subjects its networks to, i.e., that the cost functions are highly tunable, shaped by evolution and matched to the animal's ethological needs. Thus, the brain uses cost functions as a key driving force of its development, much as modern machine learning systems do.<br>(a) 大腦有足夠強力的機制來解決功勞分配問題。通過改變多層網絡中每個神經元的性質，<strong>大腦可以優化整體的成本函數。</strong><br>(b) 大腦有機制爲其各網絡精確分配不同的成本函數，即，<strong>成本函數非常可控</strong>，受到進化和動物自身生態需求的調控。<br>因此，<strong>大腦將成本函數作爲其發展的決定性驅動力</strong>，正如當今的機器學習系統一樣。</blockquote>
<p>全文太 luo 長 li 不 luo 翻 suo，在這裏只提一些看起來有趣的模型。<em>（寫了一半發現過於技術化，又捨不得刪，暫時放到文章末尾）</em></p>
<p><strong>機器學習啓發的神經科學 Machine Learning Inspired Neuroscience</strong></p>
<p>之所以提出假說，當然是爲了指導實踐&mdash;&mdash;是否有可能檢驗&ldquo;大腦中有多種多樣的成本函數來指導神經迴路的學習&rdquo;這一假設呢？</p>
<p>1. 通過猜測<strong>成本函數可以預測網絡的狀態</strong>：網絡應當處於該成本函數所指定的優化狀態。</p>
<p>2. 對成本函數的優化必然涉及到<strong>參數空間的梯度下降</strong>。或者說，在梯度下降方向的運動應當多於垂直方向的無意義旋轉。如果可以觀測神經網絡中的權重的話（看到這裏我真的笑出聲哈哈哈哈哈哈），應該可以發現權重在進行梯度下降。</p>
<p>3. 根據 1，<strong>外界干擾將使系統偏離優化狀態</strong>。通過改變突觸的權重，我們可以產生一個小的干擾，並預測系統將回歸到同一個優化狀態。這在運動領域已經開始變得可能（通過腦機接口 BMI）。</p>
<p>4. 如果我們知道哪些細胞和連接負責傳遞誤差信號，那麼可以<strong>通過刺激指定的連接來給系統強加一個用戶定義的成本函數</strong>。這將等同於把相關腦回路當做一個可訓練的深度網絡，從而研究其學習。在另一端，也可以通過腦機接口輸入新的信息來研究其行爲是否符合優化原則 (Dadarlat et al., 2015)。</p>
<p>5. 通過假想的候選成本函數來<strong>訓練人工神經網絡</strong>，可以<strong>和實際腦回路進行對比</strong>來測試假說（這一方法已經被多人應用）</p>
<p><strong>神經科學啓發的機器學習</strong></p>
<p>作者相信大腦是進化所產生的隱態機器學習機制。那麼大腦應該可以高效地優化多種數據下的多種成本函數。事實上，相比於現有的機器學習系統，大腦的<strong>硬件十分緩慢</strong>（受限於生化反應的速率）；而對<strong>非線性，不可微分，時間上隨機，基於脈衝的擁有大量反饋連接的系統</strong>如何進行優化，我們所知甚少。在系統構架層面，大腦可用的<strong>刺激展示次數少</strong>，作用於<strong>多個不同的時間框架</strong>，並採用<strong>主動學習</strong>。如果大腦果然是機器學習的範例（特別地，如果它的確解決了多層網絡的功勞分配問題），那麼我們將學到很多有用的優化算法。</p>
<p>另一方面，即使大腦並不使用反向傳遞，我們也將學到一種全新的非反向傳遞的技巧。</p>
<p>機器學習領域中已經開始研究如何用網絡產生成本函數 (Watter et al., 2015)。通過考察大腦如何在發展過程中逐漸產生和適用不同的成本函數將幫助我們在機器學習中更好地設計成本函數以及層級行爲。</p>
<p>機器學習正在發生的結構多元化亦可以從大腦結構的多元化中獲益。</p>
<blockquote>The brain combines a jumble of specialized structures in a way that works. Solving this problem <em>de novo</em> in machine learning promises to be very difficult, making it attractive to be inspired by observations about how the brain does it. <br><strong>大腦將一堆特殊結構以一種有效的方式組合在一起。</strong>在機器學習中重新解決這一問題將會非常困難&mdash;&mdash;這就是爲什麼通過觀察大腦如何做到這一點如此有吸引力。</blockquote>
<p><strong>進化將成本函數和優化算法分開了嗎？ Did Evolution Separate Cost Functions from Optimization Algorithms?</strong></p>
<p>深度學習之所以成功，是因爲其將機器學習分成了<strong>兩個部分</strong>：1 一個算法，<strong>反向傳播</strong>，用於高效而分佈式地進行優化； 2 將任何問題轉換成合適的<strong>成本函數</strong>的技巧。今天的深度學習，大部分的工作都在尋找更合適的成本函數。</p>
<p>大腦在進化中是否也找到了這一方法呢？作者認爲是的：不同的皮層區域可能分享相同的優化算法（微結構），但接受不同的數據<strong>和</strong>成本函數。事實上，針對制定皮層區域的成本函數可能是作爲輸入與數據本身一同傳遞的。</p>
<p>另一種可能則是，在皮層微結構（迴路）中，一部分連接和學習規則決定了優化算法（固定）；另一些則決定成本方程（可變）。這一思路可類比於 FPGA</p>
<p>（這裏不得不吐槽真是腦洞大開）。</p>
<p><strong>結論</strong></p>
<p>由於大腦的複雜度和多邊形，純粹的自下而上的神經數據分析面臨解釋的困難。<strong>理論框架可以被用於約束假說空間</strong>，從而允許 研究者先解決高層的原則和系統結構，再&ldquo;放大&rdquo;並解決細節。現有的自上而下的理論框架包括熵最大化，有效編碼，貝葉斯推測的可靠近似，預測誤差的最小化 *，吸引子動力學，模塊化，符號運算能力，等等。<strong>許多這類自上而下的理論本質上都是對單一計算結構的單一成本函數的優化</strong>。作者將這些假說一般化，提出<strong>多元化和發展中的成本函數羣體</strong>，以及<strong>多個專業化的子系統</strong>。</p>
<p>許多神經科學家專注於尋找&ldquo;神經編碼&rdquo;，即哪些刺激易於產生指定神經元或腦區的活動。但是如果大腦的確對成本函數進行優化，那麼我們就要注意到簡單的成本函數可以產生複雜的刺激迴應。這可能使我們轉向另一類問題。<strong>神經科學與機器學習間更加深入的對話可以幫助澄清很多問題</strong>。機器學習大部分都專注於更快地進行神經網絡中從頭到尾的梯度下降。神經科學可能爲機器學習帶來許多層面的啓示。大腦所採用的優化算法經過了數百萬年的進化。大腦可能找到了使用異質化的<strong>在發展中彼此影響的成本函數羣體</strong>通過引導無監督學習後果來簡化學習的方法。大腦中進化出的各種專門化結構可能提示我們如何提高面臨多種計算問題和跨越多個時間框架時學習系統的效率。通過<strong>尋求神經科學提供的洞見</strong>，機器學習可能邁向在一個結構異質化，標記數據有限的世界中進行學習的強人工智能。</p>
<p>在某種意義上這裏的假說與流行理論相反。並沒有單一的優化機制，單一的成本函數，單一的表現形式，或者同質化的結構。<strong>所有這些異質化的元素由優化內部產生的成本函數這一原則統一在一起。</strong>許多早期人工智能途徑都拒絕單一理論。例如，Minsky and Papert 在 《心靈的社會》中的工作，以及更廣義地，連接主義系統中遺傳預備和內部自引導的發展，強調智能需要一個由內部檢測者和評判者組成的系統，特殊化的交流與存儲機制，以及簡單控制系統的層級化組織。</p>
<p>在這些早期工作進行時，人們還不知道基於梯度的優化可以帶來強大的特徵代表和行爲政策。這裏提出的理論可以被看作是針對流行的從頭到尾的優化，重新提出異質化的方法。換句話說，這一框架可以被看成是一種<strong>由成本函數和可訓練網絡所構成的&ldquo;社會&rdquo;</strong>，從而實現類似於閔斯基在《心靈的社會》中提到的內在自舉過程。在這一觀點中，智能是由許多特別的計算結構所實現的，每一個是由其受控於發展的成本函數所訓練，而結構本身和成本函數都像超參數一樣由進化所優化 。</p>
<p>被放到末尾的一些技術細節：</p>
<p><strong>大腦可以優化成本函數 The brain can optimize cost functions</strong></p>
<p>2.1 局部自組織和優化不需要多層功勞分配 Local Self-organization and Optimization without Multi-layer Credit Assignment</p>
<p>Pehlevan and Chklovskii 2015 提出，一類 Hebbian 可塑性可被看做是提取輸入主成分(PC)的過程，從而最小化重構誤差。</p>
<p>2.2. 優化的生物基礎 Biological Implementation of Optimization</p>
<p>2.2.1. 多層網絡需要高效的梯度下降 The Need for Efficient Gradient Descent in Multi-layer Networks</p>
<p>梯度下降的重要性衆所周知，這裏不多談。知乎有一個話題就是專門講<a class="internal" href="https://www.zhihu.com/topic/19650497/hot">梯度下降</a>的。</p>
<p>2.2.2. 梯度下降的生物學近似 Biologically Plausible Approximations of Gradient Descent</p>
<p>大腦中可能用來實現對梯度下降算法近似的可能機制意外地多。其共同點爲利用反饋連接傳播誤差。一個例子是 O'Reilly 的 XCAL 算法 (O'Reilly et al., 2012)，通過本地的 Hebbian 學習法則實現了誤差的反向傳播。</p>
<p>實現反向傳播的另一個可能途徑是基於脈衝時間的可塑性 (STDP)。Hinton 就將此闡釋爲神經元可以通過脈衝速率的時間導數來編碼反向傳播所需的誤差導數 （Hinton, 2007, 2016）。</p>
<p>還有一種可能的機制則涉及到獨立於前饋連接強度的隨機反饋連接。被稱爲&ldquo;反饋對齊&rdquo;的模型中，通過突觸正規化和前饋與反饋連接的符號一致性，可以實現幾乎和反向傳播一樣好的誤差計算 (Liao et al., 2015)。</p>
<p>2.2.2.1. 時間功勞分配 Temporal credit assignment:</p>
<p>以上討論中一個重要的未解決問題是時間功勞分配：在反覆網絡 (recurrent nets) 中，爲了實現&ldquo;時域反向傳播 (BPTT)&rdquo;，機器學習使用的方法是把網絡在時間中展開 (unroll)。神經系統似乎顯然無法將自己在時間中的活動展開來進行反向傳播。</p>
<p>作者給出了幾個解決思路。其一爲通過記憶體來把時間上的功勞分配問題空間化 (例如 Weston et al., 2014)。</p>
<p>另一種方案來自於對反覆網絡監督式學習的研究。在 Sussilo and Abbott, 2009 所提出的 FORCE 模型中，網絡的輸出被鉗在指定目標，同時由網絡內部產生的隨機漲落提供反饋信號來更新權重。</p>
<p>2.2.2.2. 脈衝網絡 Spiking networks</p>
<p>2.3. 生物學習的其他原則 Other Principles for Biological Learning</p>
<p>很明顯，即使大腦確實採用了近似於反向傳播的優化算法，也不能排除其他完全不同的算法。</p>
<p>2.3.1. 利用生物神經基礎 Exploiting Biological Neural Mechanisms</p>
<p>特別地，當我們考察單個神經元的結構就會發現（這些都是老生常談）：神經元的樹突可以進行局部運算；神經元包含多個部分 (compartments)，每個神經元可以視作一個局部網絡；神經元產生動作電位時，反向（向樹突）傳播的電信號更加強烈地傳向最近活動的分支，可能簡化了功勞分配問題(K&ouml;rding and K&ouml;nig, 2000)；等等。</p>
<p>生物神經網絡一個重要的特徵是神經調節劑：同一個神經網絡根據神經調節狀態的不同，可以被看作是在多個重合的迴路之間進行切換 (Bargmann, 2012; Bargmann and Marder, 2013)。這可能允許不同迴路之間分享習得的權重。</p>
<p>2.3.2. 皮層中的學習 Learning in the Cortical Sheet</p>
<p>皮層的 6 層結構非常引人注目，有多個學習理論試圖解釋這一不斷重複的結構。通常都認爲皮層通過預測進行無監督學習(O'Reilly et al., 2014b; Brea et al., 2016)。這其中包括了直接將皮層結構對應到貝葉斯推理中信息傳遞的努力（Lee and Mumford, 2003; Dean, 2005; George and Hawkins, 2009），而另一些工作則試圖用學習理論來解釋觀測到的皮層活動。</p>
<p>這些和其他一些關於皮層運作的初步理論都超越了反向傳播。</p>
<p>* 這一理論可以參考趙思家的文章：<a class="internal" href="https://zhuanlan.zhihu.com/p/22557514">大腦無時無刻不在「預測」世界</a></p>
<p>Marblestone, A. H., Wayne, G. &amp; Kording, K. P. Toward an Integration of Deep Learning and Neuroscience. <em>Front. Comput. Neurosci.</em><strong>10,</strong> 1&ndash;61 (2016).</p>
<p>Pehlevan, C., and Chklovskii, D. B. (2015). &ldquo;Optimization theory of hebbian/anti-hebbian networks for pca and whitening,&rdquo; in <em>53rd Annual Allerton Conference on Communication, Control, and Computing</em> (Monticello, IL), 1458&ndash;1465.</p>
<p>Enel, P., Procyk, E., Quilodran, R., and Dominey, P. F. (2016). Reservoir computing properties of neural dynamics in prefrontal cortex. <em>PLoS Comput. Biol.</em> 12:e1004967. doi: 10.1371/journal.pcbi.1004967</p>
<p>O'Reilly, R. C., Wyatte, D., and Rohrlich, J. (2014b). <em>Learning through time in the thalamocortical loops</em>. arXiv:1407.3432, 37.</p>
<p>Hinton, G. (2007). &ldquo;How to do backpropagation in a brain,&rdquo; in <em>Invited Talk at the NIPS'2007 Deep Learning Workshop</em> (Vancouver, BC).</p>
<p>Hinton, G. (2016). &ldquo;Can the brain do back-propagation?,&rdquo; in <em>Invited talk at Stanford University Colloquium on Computer Systems</em> (Stanford, CA).</p>
<p>Liao, Q., Leibo, J. Z., and Poggio, T. (2015). <em>How important is weight symmetry in backpropagation?</em> arXiv:1510.05067.</p>
<p>Weston, J., Chopra, S., and Bordes, A. (2014). <em>Memory networks</em>. arXiv:1410.3916.</p>
<p>Sussillo, D., and Abbott, L. (2009). Generating coherent patterns of activity from chaotic neural networks. <em>Neuron</em> 63, 544&ndash;557. doi: 10.1016/j.neuron.2009.07.018.</p>
<p>K&ouml;rding, K., and K&ouml;nig, P. (2000). A learning rule for dynamic recruitment and decorrelation. <em>Neural Netw.</em> 13, 1&ndash;9. doi: 10.1016/S0893-6080(99)00088-X</p>
<p>Bargmann, C. I. (2012). Beyond the connectome: how neuromodulators shape neural circuits. <em>Bioessays</em> 34, 458&ndash;465. doi: 10.1002/bies.201100185.</p>
<p>Bargmann, C. I., and Marder, E. (2013). From the connectome to brain function. <em>Nat. Methods</em> 10, 483&ndash;490. doi: 10.1038/nmeth.2451</p>
<p>Brea, J., Ga&aacute;l, A. T., Urbanczik, R., and Senn, W. (2016). Prospective coding by spiking neurons. <em>PLoS Comput. Biol.</em> 12:e1005003. doi: 10.1371/journal.pcbi.1005003</p>
<p>Lee, T. S., and Mumford, D. (2003). Hierarchical Bayesian inference in the visual cortex. <em>J. Opt. Soc. Am. A Opt. Image Sci. Vis.</em> 20, 1434&ndash;1448. doi: 10.1364/JOSAA.20.001434</p>
<p>Dean, T. (2005). &ldquo;A computational model of the cerebral cortex,&rdquo; in <em>Proceedings of the 20th National Conference on Artificial Intelligence</em>(Pittsburg, PA).</p>
<p>George, D., and Hawkins, J. (2009). Towards a mathematical theory of cortical micro-circuits. <em>PLoS Comput. Biol.</em> 5:e1000532. doi: 10.1371/journal.pcbi.1000532</p>
<p>Dadarlat, M. C., O'Doherty, J. E., and Sabes, P. N. (2015). A learning-based approach to artificial sensory feedback leads to optimal integration.<em>Nat. Neurosci.</em> 18, 138&ndash;144. doi: 10.1038/nn.3883</p>
<p>Watter, M., Springenberg, J., Boedecker, J., and Riedmiller, M. (2015). &ldquo;Embed to control: a locally linear latent dynamics model for control from raw images,&rdquo; in <em>Advances in Neural Information Processing Systems</em> (Montreal, QC), 2728&ndash;2736.</p>



</div>
</div>
</div>


</div>
</div>