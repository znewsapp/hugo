+++
date = "2017-07-11T16:00:00"
title = "多虧了這種技術，即使拿到數據也查不到你的隱私了"
titleimage = "https://pic4.zhimg.com/v2-7e3a2754cbf9e83770a30132d768a85b.jpg"
ga = 071116
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">蘋果的 Differential Privacy 差分隱私技術是什麼原理？</h2>

<div class="answer">



<div class="content">
<p>原以爲差分隱私是個好小衆的話題，沒想到知乎上的討論還這麼熱烈。這是我最重要的研究方向，想盡我所能把這個學術問題說的大家都能聽懂。</p>
<p>1. 什麼是隱私？</p>
<p>講差分隱私前，我想說一下什麼是隱私。</p>
<p>其實隱私這個定義，各家有各家的說法，而且各人有各人不同的考量。目前普遍比較接受的是：&ldquo;單個用戶的某一些屬性&rdquo; 可以被看做是隱私。這個說法裏所強調的是：單個用戶。也就是說，如果是一羣用戶的某一些屬性，那麼可以不看做隱私。</p>
<p>舉個例子：醫院說，抽菸的人有更高的機率會得肺癌。這個不泄露任何隱私。但是如果醫院說，張三因爲抽菸，所以有了肺癌。那麼這個就是隱私泄露了。好，那麼進一步，雖然醫院發佈的是趨勢，說抽菸的人更高几率得肺癌。然後大家都知道張三抽菸，那麼是不是張三就會有肺癌呢？那麼這算不算隱私泄露呢？結論是不算，因爲張三不一定有肺癌，大家只是通過一個趨勢猜測的。</p>
<p>所以，<strong>從隱私保護的角度來說，隱私的主體是單個用戶，只有牽涉到某個特定用戶的才叫隱私泄露，發佈羣體用戶的信息（一般叫聚集信息）不算泄露隱私</strong>。 記得高德地圖發過一張圖，大意是開凱迪拉克的羣體喜歡去洗浴中心.......很多人說暴露隱私，其實從學術定義上來說，這個不算隱私泄露，因爲沒有牽涉到任何個體。</p>
<p>那麼我們是不是可以任意發佈聚集信息呢？倒是未必。我們設想這樣一種情況：醫院發佈了一系列信息，說我們醫院這個月有 100 個病人，其中有 10 個感染 HIV。假如攻擊者知道另外 99 個人是否有 HIV 的信息，那麼他只需要把他知道的 99 個人的信息和醫院發佈的信息比對，就可以知道第 100 個人是否感染 HIV。這種對隱私的攻擊行爲就是差分攻擊。</p>
<p>2. 差分隱私</p>
<p>差分隱私顧名思義就是防止差分攻擊了，它想做的事情就是即使你小子知道我發佈的 100 個人的信息，以及另外 99 個人的信息，你也絕對沒辦法把這兩個信息比對之後獲取第 100 個人的信息。怎麼才能做到這一點呢？差分隱私於是定義：<strong>如果你能找出一種方法讓攻擊者用某種方式查詢 100 個信息和查詢那 99 個信息得到的結果是一致的，那攻擊者就沒辦法找出那第 100 個人的信息了。</strong>但這個&ldquo;一致&rdquo; 怎麼做到呢？那就加入<strong>隨機性</strong>吧。如果查詢 100 個記錄和查詢 99 個記錄，輸出同樣值的概率是一樣的，攻擊者就無法進行差分攻擊。這裏我們就得到了差分隱私的核心思想：<strong>對於差別只有一條記錄的兩個數據集，查詢它們獲得相同值的概率非常非常的接近</strong>。Wait，不是說一致的麼？爲什麼變成了非常接近了？ 這是因爲，如果概率一樣，就表示數據集需要完全隨機化，那數據的可用性就沒有了，隱私保護也沒有意義了。所以，我們儘可能的把概率做的接近，而不是一致，以期在隱私和可用性之間找一個平衡。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/v2-1722c83e5bfd1db62987f81dcf41573d_b.jpg" alt="">
<p>上面這張圖描述了差分隱私的基本思想，對於兩個只相差一個記錄的數據集 D 和 D'來說，查詢 M 的輸出結果 S 概率應該非常接近。</p>
<p>3. 如何做到差分隱私</p>
<p>其實就是在查詢結果里加入隨機性。</p>
<p>任何一種方法，只要用在數據集上能滿足差分隱私的核心思想，那這個方法就是滿足差分隱私的。所以最常用的方法是在結果上加滿足某種分佈的噪音，使查詢結果隨機化。</p>
<p>目前常用的有兩種方法，一個是&nbsp;<strong>Laplace 機制</strong>，在查詢結果里加入 Laplace 分佈的噪音，適用於數值型輸出。例如：知乎裏有多少人是 985 大學畢業的？ 假如結果是 2000 人，那麼每一次查詢得到的結果都會稍稍有些區別，比如有很高的概率輸出 2001，也有較高概率輸出 2010， 較低概率輸出 1990，等等。</p>
<p>另外一個是<strong>指數機制</strong>，在查詢結果裏用指數分佈來調整概率，適用於非數值型輸出。例如：中國 top 3 大學是哪一所。很高概率輸出浙江大學，較高概率輸出上海交大，較低概率輸出武漢大學，很低概率輸出藍翔技校，等等。</p>
<p>4. 差分隱私應用</p>
<p>這麼說吧，任何需要保護隱私的算法裏都可以使用差分隱私。差分隱私最美麗的一點在於只要你的算法每一個步驟都滿足差分隱私的要求，那麼它可以保證這個算法的最終輸出結果滿足差分隱私，換句話說，即使攻擊者具有足夠多的背景知識，也無法在最終的輸出中找出單個人的某項屬性。</p>
<p>目前在學術上，差分隱私可以被應用在推薦系統，社交網絡，基於位置的服務，當然，也包括了蘋果的輸入系統。</p>
<p>5. 差分隱私的弱點</p>
<p>差分隱私的弱點其實很明顯：由於對於背景知識的假設過於強，需要在查詢結果中加入大量的隨機化，導致數據的可用性急劇下降。特別對於那些複雜的查詢，有時候隨機化結果幾乎掩蓋了真實結果。這也是導致目前應用不多的一個原因。</p>
<p>但差分隱私作爲一個非常漂亮的數學工具，爲隱私研究指明瞭一個發展的方向。在早期，人們很難證明我的方法保護了隱私，更無法證明究竟保護了多少隱私。現在差分隱私用嚴格的數學證明告訴人們，只要你按照我的做，我就保證你的隱私不會泄露。</p>
<p>更有意思的是，Dwork 團隊 2015 年提出應用差分隱私的想法可以解決機器學習的 over-fitting 問題，一步從隱私界跨到了 AI 界，開始搶機器學習的飯碗了。她們的論文發表在了 2015 年的 Science 上，有志於搶 AI 飯碗的同學可以瞄一下。</p>
<p><a href="http://science.sciencemag.org/content/349/6248/636">The reusable holdout: Preserving validity in adaptive data analysis</a></p>
<p>7. 最後，放一篇我在 2017 年寫的有關差分隱私的前世今生，很學術，有興趣研究這個方向的同學可以讀讀。</p>
<p><a href="http://ieeexplore.ieee.org/document/7911185/metrics">Differentially Private Data Publishing and Analysis: a Survey</a></p>
</div>
</div>




</div>


</div>
</div>