+++
date = "2017-01-05T07:00:00"
title = "如何橫掃棋壇？AlphaGo 先隨機扔了一個骰子"
titleimage = "http://pic4.zhimg.com/bfa89c8024d9a74c35bddd374944cc8b.jpg"
ga = 010507
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



<div class="headline-background">
<a class="headline-background-link" href="http://sports.sina.com.cn/go/2017-01-04/doc-ifxzkfuh5258431.shtml">
<div class="heading">相關新聞</div>
<div class="heading-content">DeepMind 官方確認 Master 身份 望以後繼續探索</div>
<i class="icon-arrow-right"></i>
</a>
</div>

</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">DeepMind 研發的圍棋 AI AlphaGo 系統是如何下棋的？</h2>

<div class="answer">



<div class="content">
<p><strong>左右互搏，青出於藍而勝於藍？</strong></p>
<p><strong>&mdash;阿爾法狗原理解析</strong></p>
<p>19 年前計算機擊敗國際象棋冠軍卡斯帕羅夫的情景還歷歷在目，現在計算機又要來攻克圍棋了嗎！？</p>
<p>虛竹在天龍八部裏自填一子，無意中以&ldquo;自殺&rdquo;破解&ldquo;珍籠&rdquo;棋局，逍遙子方纔親傳掌門之位。難道以後&ldquo;阿爾法狗&rdquo;要出任逍遙派掌門了？</p>
<p>1933 年，東渡日本 19 歲的吳清源迎戰當時的日本棋壇霸主、已經 60 歲的本因坊秀哉，開局三招即是日本人從未見過的三三、星、天元佈陣，快速進擊逼得對方連連暫停&ldquo;打卦&rdquo;和弟子商量應對之策。隨後以&ldquo;新佈局&rdquo;開創棋壇新紀元。難道阿爾法狗會再造一個&ldquo;新新佈局&rdquo;？</p>
<p>作爲一個關心人工智能和人類命運的理科生，近些天刷了好些報道，記者們說&ldquo;阿爾法狗是個&lsquo;價值神經網絡&rsquo;和&lsquo;策略神經網&rsquo;絡綜合蒙特卡洛搜索樹的程序&rdquo;，但我覺得光知道這些概念是不夠的。我想看看&ldquo;阿爾法狗&rdquo;的廬山真面目。</p>
<p><strong>準備好棋盤和腦容量，一起來探索吧？</strong></p>
<img class="content-image" src="http://pic4.zhimg.com/70/974359da2f3a99d8a3d6600f5580c553_b.jpg" alt="">
<p>圍棋棋盤是 19x19 路，所以一共是 361 個交叉點，每個交叉點有三種狀態，可以用 1 表示黑子，-1 表示白字，0 表示無子，考慮到每個位置還可能有落子的時間、這個位置的氣等其他信息，我們可以用一個 361 * n 維的向量來表示一個棋盤的狀態。我們把一個棋盤狀態向量記爲 s。</p>
<p>當狀態 s 下，我們暫時不考慮無法落子的地方，可供下一步落子的空間也是 361 個。我們把下一步的落子的行動也用 361 維的向量來表示，記爲 a。</p>
<p>這樣，設計一個圍棋人工智能的程序，就轉換成爲了，任意給定一個 s 狀態，尋找最好的應對策略 a，讓你的程序按照這個策略走，最後獲得棋盤上最大的地盤。</p>
<p>如果你想要設計一個特別牛逼驚世駭俗的圍棋程序，你會從哪裏開始呢？對於在谷歌 DeepMind 工作的黃士傑和他的小夥伴而言，第一招是：</p>
<p><strong>&ldquo;</strong><strong>深度卷積神經網絡</strong><strong>&rdquo;</strong></p>
<img class="content-image" src="http://pic3.zhimg.com/70/2e3c8284f817fb293fc1624e9a6c35aa_b.jpg" alt="">
<p>深度卷積神經網絡早在 98 年就攻克了手寫數字識別，近些年在人臉識別、圖像分類、天氣預報等領域無往而不利，接連達到或超過人類的水平，是深度學習火遍大江南北的急先鋒。我們現在看到的 Picasa 照片自動分類，Facebook 照片識別好友，以及彩雲天氣高精度天氣預報（軟廣出現，不要打我）都是此技術的應用。這等天賜寶物，如果可以用來下圍棋，豈不是狂拽酷炫吊炸天？</p>
<p>所以 2015 年黃士傑發表在 ICLR 的論文[3]一上來就使出了&ldquo;深度神經網絡&rdquo;的殺招，從網上的圍棋對戰平臺 KGS（外國的 qq 遊戲大廳）可以獲得人類選手的圍棋對弈的棋局。觀察這些棋局，每一個狀態 s，都會有一個人類做出的落子 a，這不是天然的訓練樣本&lt;s,a&gt;嗎？如此可以得到 3000 萬個樣本。我們再把 s 看做一個 19x19 的二維圖像（具體是 19x19 x n，n 是表示一些其他 feature），輸入一個卷積神經網絡進行分類，分類的目標就是落子向量 a&rsquo;，不斷訓練網絡，儘可能讓計算機得到的 a&rsquo;接近人類高手的落子結果 a，不就得到了一個模擬人類棋手下圍棋的神經網絡了嗎？</p>
<p>於是我們得到了一個可以模擬人類棋手的策略函數 P_human，給定某個棋局狀態 s，它可以計算出人類選手可能在棋盤上落子的概率分佈 a = P_human(s)，如下圖：</p>
<img class="content-image" src="http://pic3.zhimg.com/70/762294d8ac76a560d067d86e0a596336_b.jpg" alt="">
<p>紅圈就是 P_human 覺得最好的落子方案。每一步都選擇概率最高的落子，對方對子後再重新計算一遍，如此往復就可以得到一個棋風類似人類的圍棋程序。</p>
<p>這個基於&ldquo;狂拽酷炫&rdquo;深度學習的方案棋力如何呢？</p>
<p><strong>不咋地</strong>。黃士傑說 P_human 已經可以和業餘 6 段左右的人類選手過招，互有勝負，但還未能超過當時最強的電腦程序 CrazyStone[1,5]，距離人類頂尖玩家就差得更遠了。</p>
<p>所以，爲求更進一步，黃士傑打算把 P_human 和 CrazyStone 的算法結合一下，師夷長技以制夷，先擊敗所有的其他圍棋 AI 再說。</p>
<p>等等，CrazyStone 的算法是什麼？</p>
<p>哦，那個算法是黃士傑的老師 Remi Coulum 在 2006 年對圍棋 AI 做出的另一個重大突破：</p>
<p><strong>&ldquo;</strong><strong>MCTS</strong><strong>，蒙特卡洛搜索樹</strong><strong>&rdquo;</strong></p>
<img class="content-image" src="http://pic3.zhimg.com/70/51ca11128a5aafdc2996a263dd9fabf6_b.jpg" alt="">
<p>蒙特卡洛搜索樹（Monte-Carlo Tree Search）是一種&ldquo;大智若愚&rdquo;的方法。面對一個空白棋盤 S0，黃士傑的老師 Coulum 最初對圍棋一無所知，便假設所有落子方法分值都相等，設爲 1。然後扔了一個骰子，從 361 種落子方法中隨機選擇一個走法 a0。Coulum 想象自己落子之後，棋盤狀態變成 S1，然後繼續假設對手也和自己一樣二逼，對方也扔了一個篩子，隨便瞎走了一步，這時棋盤狀態變成 S2，於是這兩個二逼青年一直扔骰子下棋，一路走到 Sn，最後肯定也能分出一個勝負 r，贏了就 r 記爲 1，輸了則爲 0，假設這第一次 r=1。這樣 Coulum 便算是在心中模擬了完整的一盤圍棋。</p>
<p>Coulum 心想，這樣隨機扔骰子也能贏？運氣不錯啊，那把剛纔那個落子方法（S0,a0）記下來，分值提高一些：</p>
<ul>
<li>新分數= 初始分 + r</li>
</ul>
<p>我剛纔從（S0, a0）開始模擬贏了一次，r=1，那麼新分數=2，除了第一步，後面幾步運氣也不錯，那我把這些隨機出的局面所對應落子方法(Si,ai)的分數都設爲 2 吧。然後 Coulum 開始做第二次模擬，這次扔骰子的時候 Coulum 對圍棋已經不是一無所知了，但也知道的不是太多，所以這次除（S0, a0）的分值是 2 之外，其他落子方法的分數還是 1。再次選擇 a0 的概率要比其他方法高一點點。</p>
<p>那位假想中的二逼對手也用同樣的方法更新了自己的新分數，他會選擇一個 a1 作爲應對。如法炮製，Coulum 又和想象中的對手又下了一盤稍微不那麼二逼的棋，結果他又贏了，Coulum 於是繼續調整他的模擬路徑上相應的分數，把它們都 +1。隨着想象中的棋局下得越來越多，那些看起來不錯的落子方案的分數就會越來越高，而這些落子方案越是有前途，就會被更多的選中進行推演，於是最有&ldquo;前途&rdquo;的落子方法就會&ldquo;涌現&rdquo;出來。</p>
<p>最後，Coulum 在想象中下完 10 萬盤棋之後，選擇他推演過次數最多的那個方案落子，而這時，Coulum 才真正下了第一步棋。</p>
<p>蒙特卡洛搜索樹華麗轉身爲相當深刻的方法，可以看到它有兩個很有意思的特點：</p>
<p>1）沒有任何人工的 feature，完全依靠規則本身，通過不斷想象自對弈來提高能力。這和深藍戰勝卡斯帕羅夫完全不同，深藍包含了很多人工設計的規則。MCTS 靠的是一種類似遺傳算法的自我進化，讓靠譜的方法自我涌現出來。讓我想起了卡爾文在《大腦如何思維》中說的思維的達爾文主義[6]。</p>
<p>2）MCTS 可以連續運行，在對手思考對策的同時自己也可以思考對策。Coulum 下完第一步之後，完全不必要停下，可以繼續進行想象中的對弈，直到對手落子。Coulum 隨後從對手落子之後的狀態開始計算，但是之前的想象中的對弈完全可以保留，因爲對手的落子完全可能出現在之前想象中的對弈中，所以之前的計算是有用的。這就像人在進行對弈的時候，可以不斷思考，不會因爲等待對手行動而中斷。這一點 Coulum 的程序非常像人，酷斃了。</p>
<p>但黃士傑很快意識到他老師的程序仍然有侷限：初始策略太簡單。我們需要更高效地扔骰子。</p>
<p>如何更高效的扔骰子呢？</p>
<p>用 P_human()來扔。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/c7a7359c45778f68036706e6fbc42d19_b.jpg" alt="">
<p>黃士傑改進了 MCTS，一上來不再是二逼青年隨機擲骰子，而是先根據 P_human 的計算結果來得到 a 可能的概率分佈，以這個概率來挑選下一步的動作。一次棋局下完之後，新分數按照如下方式更新：</p>
<ul>
<li>新分數= 調整後的初始分 + 通過模擬得到的贏棋概率</li>
</ul>
<p>如果某一步被隨機到很多次，就應該主要依據模擬得到的概率而非 P_human。</p>
<p>所以 P_human 的初始分會被打個折扣：</p>
<ul>
<li>調整後的初始分= P_human/（被隨機到的次數 + 1）</li>
</ul>
<p>這樣就既可以用 P_human 快速定位比較好的落子方案，又給了其他位置一定的概率。看起來很美，然後實際操作中卻發現：&ldquo;然並卵&rdquo;。因爲，P_human()計算太慢了。</p>
<p>一次 P_human()計算需要 3ms，相對於原來隨機扔骰子不到 1us，慢了 3000 倍。如果不能快速模擬對局，就找不到妙招，棋力就不能提高。所以，黃士傑訓練了一個簡化版的 P_human_fast()，把神經網絡層數、輸入特徵都減少，耗時下降到了 2us，基本滿足了要求。先以 P_human()來開局，走前面大概 20 多步，後面再使用 P_human_fast()快速走到最後。兼顧了準確度和效率。</p>
<p>這樣便綜合了深度神經網絡和 MCTS 兩種方案，此時黃士傑的圍棋程序已經可以戰勝所有其他電腦，雖然距離人類職業選手仍有不小的差距，但他在 2015 年那篇論文的最後部分信心滿滿的表示：&ldquo;我們圍棋軟件所使用的神經網絡和蒙特卡洛方法都可以隨着訓練集的增長和計算力的加強（比如增加 CPU 數）而同步增強，我們正前進在正確的道路上。&rdquo;</p>
<p>看樣子，下一步的突破很快就將到來。同年 2 月，黃士傑在 Deepmind 的同事在頂級學術期刊 nature 上發表了&ldquo;用神經網絡打遊戲&rdquo;的文章[2]。這篇神作，爲進一步提高 MCTS 的棋力，指明瞭前進的新方向：</p>
<p><strong>&ldquo;</strong><strong>左右互搏，自我進化</strong><strong>&rdquo;</strong></p>
<p>紅白機很多人小時候都玩過，你能都打通嗎？黃士傑的同事通過&ldquo;強化學習&rdquo;方法訓練的程序在類似紅白機的遊戲機上打通了 200 多個遊戲，大多數得分都比人類還好。</p>
<img class="content-image" src="http://pic4.zhimg.com/70/db76a9c4ec7bdf34bf6e5776fdae8a07_b.jpg" alt="">
<p>&ldquo;強化學習&rdquo;是一類機器學習方法，Agent 通過和環境 s 的交互，選擇下一步的動作 a，這個動作會影響環境 s，給 Agent 一個 reward，Agent 然後繼續和環境交互。遊戲結束的時候，Agent 得到一個最後總分 r。這時我們把之前的環境狀態 s、動作 a 匹配起來就得到了一系列&lt;s,a&gt;，設定目標爲最後的總得分 r，我們可以訓練一個神經網絡去擬合在狀態 s 下，做動作 a 的總得分。下一次玩遊戲的時候，我們就可以根據當前狀態 s，去選擇最後總得分最大的動作 a。通過不斷玩遊戲，我們對&lt;s,a&gt;下總得分的估計就會越來越準確，遊戲也玩兒得越來越好。</p>
<p>打磚塊遊戲有一個祕訣：把球打到牆的後面去，球就會自己反彈得分。強化學習的程序在玩了 600 盤以後，學到這個祕訣：球快要把牆打穿的時候評價函數 v 的分值就會急劇上升。</p>
<img class="content-image" src="http://pic1.zhimg.com/70/bb8f60e5d54509a75327f607fa2b4778_b.jpg" alt="">
<p>黃士傑考慮給圍棋也設計一個評價函數 v(s)，在 P_human()想象自己開局走了 20 多步之後，不需要搜索到底，如果有一個 v(s)可以直接判斷是否能贏，得到最後的結果 r，這樣肯定能進一步增加 MCTS 的威力。</p>
<p>黃士傑已經有了國外的 qq 遊戲大廳 KGS 上的對局，但是很遺憾這些對局數量不夠，不足以得到局面評價函數 v。但是沒關係，我們還可以左右互搏自對弈創造新的對局。</p>
<p>機器學習的開山鼻祖 Samuel 早在 1967 年就用自對弈的方法來學習國際跳棋[7]，而之前的蒙特卡洛搜索樹也是一個自對弈的過程。但是現在黃士傑不僅有一個從人類對弈中學習出的 P_human 這樣一個高起點，而且有一個神經網絡可以從對弈樣本中學習，有理由相信這次會有更好的結果。</p>
<img class="content-image" src="http://pic2.zhimg.com/70/70380bee21aac639530c33cc32429f4d_b.jpg" alt="">
<p>先用 P_human 和 P_human 對弈，比如 1 萬局，就得到了一萬個新棋譜，加入到訓練集當中，訓練出 P_human_1。然後再讓 P_human_1 和 P_human_1 對局，得到另外一萬個新棋譜，這樣可以訓練出 P_human_2，如此往復，可以得到 P_human_n。P_human_n 得到了最多的訓練，棋力理應比原來更強。我們給最後這個策略起一個新名字：P_human_plus。這時，再讓 P_human_plus 和 P_human 對局，在不用任何搜索的情況下勝率可達 80%，不加任何搜索策略的 P_human_plus 和開源的 MCTS 相比也有 85% 的勝率。自對弈方法奏效了。</p>
<p>既然 P_human_plus 這麼強，我們先代入到 MCTS 中試試，用 P_human_plus 來開局，剩下的用 P_human_fast。可惜，這樣的方法棋力反而不如用 P_human。黃士傑認爲是因爲 P_human_plus 走棋的路數太集中，而 MCTS 需要發散出更多的選擇纔好。看來，P_human_plus 練功還是太死板，還沒有進入無招勝有招的境界。</p>
<p>沒關係，黃士傑還有局面評價函數 v(s)這一招，有了 v(s)，如果我可以一眼就看到&ldquo;黑棋大勢已去&rdquo;，我就不用 MCTS 在想象中自我對弈了。但考慮到 P_human_plus 的招法太過集中，黃士傑在訓練 v( )的時候，開局還是先用 P_human 走 L 步，這樣有利於生成更多局面。黃士傑覺得局面還不夠多樣化，爲了進一步擴大搜索空間，在 L+1 步的時候，乾脆完全隨機擲一次骰子，記下這個狀態 SL+1，然後後面再用 P_human_plus 來對弈，直到結束獲得結果 r。如此不斷對弈，由於 L 也是一個隨機數，我們就得到了開局、中盤、官子不同階段的很多局面 s，和這些局面對應的結果 r。有了這些訓練樣本&lt;s,r&gt;，還是使用神經網絡，把最後一層的目標改成迴歸而非分類，黃士傑就可以得到一個 v( )函數，輸出贏棋的概率。</p>
<img class="content-image" src="http://pic4.zhimg.com/70/cbf57ac5d7e261ae0f1ead0ecd01bbab_b.jpg" alt="">
<p>v( )可以給出下一步落子在棋盤上任意位置之後，如果雙方都使用 P_human_plus 來走棋，我方贏棋的概率。如果訓練 v()的時候全部都使用 P_human 不用 P_human_plus 呢？實驗表明基於 P_human_plus 訓練的 v，比基於 P_human 訓練的 v&rsquo;，棋力更強。強化學習確實有效。</p>
<p>萬事俱備，只欠東風。準備好 P_human()，MCTS，以及評價函數 v()，黃士傑和小夥伴們繼續進擊，向着可以和人類專業選手過招的圍棋 AI 前進：</p>
<p><strong>&ldquo;</strong><strong>阿爾法狗</strong><strong>&rdquo;</strong></p>
<img class="content-image" src="http://pic3.zhimg.com/70/20e73927650d7f39c49d2b913f91c18e_b.jpg" alt="">
<p>黃士傑準備在 MCTS 框架之上融合局面評估函數 v()。這次還是用 P_human 作爲初始分開局，每局選擇分數最高的方案落子，下到第 L 步之後，改用 P_human_fast 把剩下的棋局走完，同時調用 v(SL)，評估局面的獲勝概率。然後按照如下規則更新整個樹的分數：</p>
<ul>
<li>新分數= 調整後的初始分 + 0.5 * 通過模擬得到的贏棋概率 + 0.5 * 局面評估分</li>
</ul>
<p>前兩項和原來一樣，如果待更新的節點就是葉子節點，那局面評估分就是 v(SL)。如果是待更新的節點是上級節點，局面評估分是該節點所有葉子節點 v()的平均值。</p>
<p>如果 v()表示大局觀，&ldquo;P_human_fast 模擬對局&rdquo;表示快速驗算，那麼上面的方法就是大局觀和快速模擬驗算並重。如果你不服，非要做一個 0.5: 0.5 之外的權重，黃士傑團隊已經實驗了目前的程序對陣其他權重有 95% 的勝率。</p>
<p>以上，便是阿爾法狗的廬山真面目。</p>
<img class="content-image" src="http://pic3.zhimg.com/70/fbafd1cb5d6ae44715a47d5309d0b95e_b.jpg" alt="">
<p>上圖演示了阿爾法狗和樊麾對弈時的計算過程，阿爾法狗執黑，紅圈是阿爾法狗實際落子的地方。1、2、3 和後面的數字表示他想象中的之後雙方下一步落子的地方。白色方框是樊麾的實際落子。在覆盤時，樊麾覺得位置 1 的走法更好。</p>
<p>深度學習、蒙特卡洛搜索樹，自我進化三招齊出，所有其他圍棋 ai 都毫無還手之力。99% 的勝率不說，&ldquo;阿爾法狗&rdquo;還可以在讓四子的情況下以 77% 的勝率擊敗 crazystone。&ldquo;阿爾法狗&rdquo;利用超過 170 個 GPU，粗略估算超過 800 萬核並行計算，不僅有前期訓練過程中模仿人類，自我對弈不斷進化，還有實戰時的模擬對局可以實時進化，已經把現有方法發揮到了極限，是目前人工智能領域絕對的巔峯之作。</p>
<p><strong>後記</strong></p>
<p>圍棋是 NP-hard 問題，如果用一個原子來存儲圍棋可能的狀態，把全宇宙的原子加起來都不夠儲存所有的狀態。於是我們把這樣的問題轉換爲尋找一個函數 P，當狀態爲 S 時，計算最優的落子方案 a = P(s)。我們看到，無論是&ldquo;狂拽酷炫&rdquo;的深度學習，還是&ldquo;大智若愚&rdquo;的 MCTS，都是對 P(s)的越來越精確的估計，但即使引入了&ldquo;左右互搏&rdquo;來強化學習，黃士傑和團隊仍然做了大量的細節工作。所以只有一步一個腳印，面對挑戰不斷拆解，用耐心與細心，還有辛勤的汗水，才能取得一點又一點的進步，而這些進步積累在一起，終於讓計算機達到並超過了人類職業選手的水平。</p>
<img class="content-image" src="http://pic1.zhimg.com/70/988b28e08f03ce839e4d5abef07f07b4_b.jpg" alt="">
<p>因爲一盤棋走一步需要 3ms（P_human_plus 遍歷整個棋盤的時間），谷歌用大規模集羣進行並行化計算，自我對弈 3000 萬盤棋生成訓練集只需要一天左右的時間[4]，所以如果對弈更多棋局可以提高棋力的話，黃士傑他們早就做了。目前的方案可能已經達到了 CNN 網絡能力的極限。完整的阿爾法狗不僅需要生成訓練集，還要用訓練集來生成局面評估函數 v()，而這還使用了兩週時間，一局比賽需要花掉 4 個小時，自我對局速度不夠快，這也許是阿爾法狗並沒有能夠完全使用強化學習，而僅僅是在整個過程的一小部分使用左右互搏的原因。左右互博用的還不夠多，這是一個遺憾。</p>
<p>如果存在一個&ldquo;圍棋之神&rdquo;，一個已經窮盡了所有的圍棋步法的&ldquo;上帝&rdquo;，那他每一步都是最優應對。一些頂尖棋手在接受採訪時表示[8]，&ldquo;圍棋之神&rdquo;對戰人類選手可能還有讓 4 子的空間，也就是說，就算下贏了人類，計算機也還有很大進步的空間。</p>
<p>面對一個如此高難度的問題，計算機和人類都無法在有限時間內找到完全的規律（柯潔和李世乭比賽是一人有 3 小時時間思考，阿爾法狗今年 3 月和李世乭進行的比賽則是每人 2 小時）。計算機和人都是在對問題做抽象，然後搜索最佳策略。要下好圍棋所需要的能力已經接近人類智力的極限：要有大局觀、要懂得取捨、還要會精打細算，治理一個國家也不過如此。計算機可以學會圍棋，就能學會很多一樣難度的技能。在未來，也許圍棋、自動駕駛、同聲傳譯都會被一一攻克。甚至在數論、量子場論等領域，深度學習和搜索相結合，可能也會帶給我們更多驚喜，比如攻克&ldquo;哥德巴赫猜想&rdquo;。</p>
<p>那麼，人工智能是否真的會很快登頂呢？</p>
<p>雖然在智力方面 AI 有希望登峯造極，但高智商只是人類衆多能力的一個方面。吳清源先生在方寸之間縱橫無敵，但仍然漂泊一生，被命運推着前進。早年他做段祺瑞的門客，棋盤上把段祺瑞打的落花流水，弄得下人都沒有早飯吃；後來東渡日本，三易國籍，留下許多遺憾。如果把&ldquo;強人工智能&rdquo;比作一個天才少年，雖然智商爆表，但其他方面還需要我們悉心加以引導。創造出&ldquo;德才兼備，匡扶濟世&rdquo;的人工智能，纔是我輩真正應該努力實現的目標。</p>
<p><strong>一起加油吧，科學少年們！</strong></p>
<p><strong>To the infinity and beyond !</strong></p>
<p>參考文獻：</p>
<p>1, EfficientSelectivity and Backup Operators in Monte-Carlo Tree Search</p>
<p>2, Human-level control through deep reinforcementlearning</p>
<p>3, Move Evaluation In GO Using Deep Convolutional Neural Networks</p>
<p>4. Masteringthe Game of Go with Deep Neural Networks and Tree Search</p>
<p>5. A Survey ofMonte Carlo Tree Search Methods</p>
<p>6. 大腦如何思維&mdash;智力演化的今昔</p>
<p>7. Some Studies in Machine LearningUsing the Game of Checkers.II-Recent Progress</p>
<p>8.<a href="http://www.sbanzu.com/topicdisplay.asp?BoardID=-4&amp;Page=1&amp;UserName=takami&amp;TopicID=4024042">圍棋之神存在的話，可以讓你几子？</a></p>
<p>&nbsp;</p>
</div>
</div>




</div>


</div>
</div>