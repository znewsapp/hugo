+++
date = "2017-01-25T19:00:00"
title = "自動駕駛的汽車是怎麼「算命」的？"
titleimage = "http://pic2.zhimg.com/87f24431d32cd5724344081b37e09959.jpg"
ga = 012519
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">自動駕駛汽車在危險情況下可以不計後果地保護自己的主人嗎？</h2>

<div class="answer">



<div class="content">
<p>經常看到自動駕駛相關問題下面會有吃瓜羣衆提出類似於有軌電車難題的問題， 我稱之爲場景 A：</p>
<p>一個自動駕駛汽車在自己的車道上行駛，等他發現前面路上有五個人的時候已經很遲了，他只有兩個選擇：</p>
<p>1.不拐彎直接撞上路上的兩個人；</p>
<p>2.拐個彎撞上在車道旁邊人行道上乖乖走的一個人。</p>
<p>題主的問題跟電車難題有相似之處，我稱之爲場景 B：</p>
<p>假如我的自動駕駛汽車發現前面路上有 10 個小朋友在過馬路，他只有兩個選擇：</p>
<p>策略 1：不顧世俗的非議撞上去保我的命。</p>
<p>策略 2：同情心爆發拐個彎帶着我一起衝到路邊的河裏面</p>
<p>有這個問題的人通常可能是把強人工智能當作自動駕駛所使用的人工智能，意思就是自動駕駛汽車怎麼選擇， 會綜合考慮到法律，道德，同情心，忠誠等等因素。但是在無人駕駛攻城獅眼裏， 事情並不是這樣子的。無人駕駛中的人工智能只是弱人工智能，他根本沒有法律道德之類的這些意識。實際上這裏的決策問題可以看作路徑規劃問題，轉還是不轉就代表着無人駕駛汽車選擇了什麼樣的行走路徑。 而傳統的路徑規劃問題，無人駕駛在考慮當前周邊環境（車道十個小朋友，車道邊有條河）以及法律法規甚至倫理道德等所有方面做出決策的過程， 實際上就是把這些各個因素用數學表示出來， 在這些因素的限制框架下，求解一個最優化問題。也就是說， 弱 人工智能下的自動駕駛車輛所作的決策，取決於攻城獅實現決策算法的時候是怎麼建模的，若果在模型中駕駛員 的安全更重要， 車就會撞上去；如果模型優化的是總的傷亡人數，車就會衝進去河裏。</p>
<p>當然，這裏沒有考慮端到端深度增強學習訓練出來的駕駛策略，也就是輸入直接是攝像頭的圖片，輸出就是汽 車的行駛策略， 沒有中間檢測行人，建模法律法規道德倫理的過程。 至於神經網絡內部有沒有自己知道那裏是 行人（以神經網絡的檢測能力，應該是有的）， 有沒有考慮到法律法規，開發人員是不知道的。就像阿狗下一 步下在哪裏，沒有人知道爲什麼。另外，這種極其罕見的例子， 就算讓特斯拉全球幾萬免費義務測試車不知道 是幸運還是不幸運碰見了一次，拿來訓練神經網絡， 對神經網絡也沒有任何影響， 因爲跟正常行駛的比起來， 數量少太多太多。 所以， 目前來看，沒有自動駕駛車輛會完全採用端到端的訓練結果。 就算是 mobileye 也只 是把端到端的訓練結果作爲車道保持算法的輸入之一， 在卡爾曼濾波的框架下根別的檢測結果結合起來。</p>
<p>所以，這裏簡要的介紹一下， 在傳統控制或者路徑規劃算法的框架下，攻城獅怎麼建模這個問題，怎麼確保他 們開發的策略能在人們的法律以及道德倫理的審視下被普遍接受。</p>
<p>法律以及道德的約束，比如雙黃線，比如不能撞人，比如撞豬比撞人好等等，在最優控制（optimal control） 的框架下都可以轉化爲約束（constraint）以及損失函數（cost function）。</p>
<p><strong>約束</strong></p>
<p>約束很容易理解，車不能撞人，車不能越過雙簧線，車不能闖紅燈。</p>
<p>如下圖</p>
<img class="content-image" src="http://pic1.zhimg.com/70/v2-33f1080826727dd7c973e9e91a8d7e08_b.jpg" alt="">
<p>假如雙黃線的 X 座標爲 0。假設 X_auto 是車最左邊的位置，那麼不能越過雙黃線這一法律法規用數學的語 言表示就是 X_auto &gt;= 0 。</p>
<img class="content-image" src="http://pic3.zhimg.com/70/v2-c51f8e65d2f4e56db676384f77eff41a_b.jpg" alt="">
<p>同樣的，假如車前面有個行人，車和人的距離是 d， 那麼車不能撞人的這一約束就可以轉化爲 d &gt;= 0.5 （簡化 起見，不考慮車轉彎）。</p>
<p><strong>損失函數</strong></p>
<p>損失函數的概念用在人工智能，自動控制等的方方面面。損失函數的自變量可以是車輛的下一步行爲決策以及位 置速度等等。 損失函數最小是算法的優化目標。具體確定損失函數的形式也需要很多具體知識以及經驗以及應 用背景。Uber 的自動駕駛 Taxi 可能希望乘客的體驗儘可能舒適， 那麼就可以定爲：加速度變化大的決策損失函 數大，轉彎大的策略損失函數大。而做自動卡車的 Otto 可能就希望他們跟路上的車啊行人保持儘可能大的距離； 對於一個龐大的卡車來說， 跟別人保持距離是很必要的，而裏面的人（如果有的話）的舒適感就比較次要了， 看看柏林發生的就知道了，據說那個卡車上還安裝了自動剎車設備。</p>
<img class="content-image" src="http://pic1.zhimg.com/70/v2-3cd6db88bf4687b07b06d71107f1c3ec_b.jpg" alt="">
<p>這裏假設損失函數就是 F（轉角速度，加速度。 對於 Uber 和 Otto 來說，損失函數 F 的形式不一樣，但是自變量都 是轉角速度和加速度。</p>
<p>所以路徑規劃或者最優控制解決了這樣一個問題，根據法律法規道德規範，制定了一系列的約束；根據廠家開發 者乘客等等各種亂七八糟的期望定了一個損失函數。 然後用一些數學的方法在以上框架下求最小值，能使損失 函數最小的轉角速度和加速度就可以看作是自動駕駛汽車下一步的策略。</p>
<p>根據我們日常的駕駛經驗，以上算法基本上可以涵蓋百分之九十五的情形。 但是假如發生瞭如下情況：</p>
<img class="content-image" src="http://pic3.zhimg.com/70/v2-fc65e3827b434b597576a214756e0342_b.jpg" alt="">
<p>兩邊都是雙黃線，前面一個人站在路中間等碰瓷 ，剎車來不及了。我們第一個約束說，雙黃線不能越過去；我們第二個約束說，不能撞人。 在這種情況下，我們的約束不能被全部滿足，數學就叫做 infeasible， 沒有可行解。 當然自動駕駛汽車不能說， 完了，難得世間兩全事，不越雙黃不撞人，我不知道怎麼辦了，你自己看着辦吧。</p>
<p>常見的解決辦法，用在深度學習出現之前的王者，支持向量機裏面的技巧，就是 softconstraint（軟約束？怪怪的）。 就是說平時一切風調雨順的時候， 所有這些約束都滿足；但是問題變得無解的時候，算法可以違背這些 softconstraint， 當然要付出比較大的懲罰（損失函數值極大增加）。比如說， 壓雙黃線損失函數增加 20， 撞人損失函數增加 10000， 那算法當然決定越過雙黃線。</p>
<p>據說奔馳做 berthabenz 無人駕駛測試的時候，算法裏面定的是完全不能違反交通規則， 也就是硬約束， 然後一個場景是一輛卡車停在前面卸貨，左邊是雙黃線，要開過去車必須要部分越線， 於是車就停下來了， 只能手動駕駛通過。</p>
<p>所以機器人三大定律的第一定律 -- 機器人不得傷害人類， 變得不再是不可違背了，一切取決於當沒有滿足所有法律法規道德規範所確立的約束條件的可行解的時候， 算法中爲違反每個約束條件所定下來的損失函數懲罰數值； 或者必須要有約束被違背的時候，哪些約束變成軟約束，哪些不變。在場景 A 中， 撞人必須成爲軟約束， 如果撞一個人的懲罰是 10000，偏離道路的懲罰是 100， 那策略 2 的損失函數值是 10100；而撞五個人的損失函數值可能是 50000（如果考慮疊加的話）。 基於損失函數最小的原則， 自動駕駛汽車會選擇撞上人行道上的人。 而在情景 2 中，很有可能保護車主還是作爲硬約束，撞人變爲軟約束了， 無論懲罰值是多少都會選擇撞上去。"不計後果的保護主人"聽着很擬人化，好像自動駕駛汽車忠心耿耿，其實只是算法裏面所定下來的各個約束被違反時的懲罰數值以及轉化爲軟約束的優先順序所決定的罷了。</p>
</div>
</div>




</div>


</div>
</div>