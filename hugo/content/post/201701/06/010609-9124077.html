+++
date = "2017-01-06T09:00:00"
title = "接下來，讓我們歡迎今天的 DJ——統治棋壇的 AlphaGo"
titleimage = "http://pic2.zhimg.com/10d1fb0f511757e21220ab4c38539fc5.jpg"
ga = 010609
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">AlphaGo 在圍棋上獲得成功後，人工智能的下一個目標會是同爲「有限元素組合創作」的音樂嗎？</h2>

<div class="answer">



<div class="content">
<p>終於有人來說音樂了。</p>
<p>我想從兩個方面來談談這個問題，<strong>一個是計算機對於音樂的理解，一個是計算機通過學習來進行音樂創作。</strong></p>
<p>首先是對於音樂的理解，</p>
<p>把神經網絡應用在音樂上其實並不是一個非常新的話題，很早以前開始，在音頻檢索領域，人們就嘗試用神經網絡給各種各樣的音樂元素（和絃，基頻等等）建模，但是和語音識別一樣，受限於計算機的運算能力，一直沒有辦法取得突破性的進展，在相當長的一段時間裏人們一直還是在調參數和傅里葉變換中來回折騰。直到最近開始，隨着計算機運算能力的不斷提高，人們開始重新意識到深度學習在音樂上的應用。</p>
<p>大約兩年前的時候，一位研究 Deep Learning 的博士生來我們組實習，利用深度學習的方法，在<strong>完全基於</strong>音樂音頻信號的基礎上訓練了我們曲庫裏的 6000 萬首歌，在沒有任何人爲標籤和協通濾波的輔助下，取得了相當驚人的效果。</p>
<p>我們知道，一般做歌曲推薦的時候，我們通常依賴於協同濾波（Collaborative Filtering），也就是我們假設聽的歌曲和聽的人之間有很高的重合度。或者更直接而 naive 的方法就是打標籤，給歌曲打上一些諸如曲風，場景，情緒一類的語義化標籤。但是這兩者都有不可避免的問題，前者會有嚴重的長尾效應，沒人聽過的歌曲永遠沒人聽，越是流行的歌曲被推薦的機率越高；而後者，如果標籤質量夠高自然沒什麼問題，但是這免不了要耗費巨大的人力物力來手動標註（比如 Pandora），也並不是每個公司都有興趣這麼做的，你也無法始終保持標籤的質量。</p>
<p>但是經過深度學習後的計算機不是這麼想問題的，在高維空間裏，每一首歌對於計算機來說就是一個個向量，接下來，你用最簡單的聚類也好，建立二叉樹模型也好。在聽覺上相似的歌曲，一定會在高維空間裏找到彼此。</p>
<p>在他的報告裏面有一些很有意思的現象，比如：音樂性上特別有特點的兩首歌（比如都有小提琴和 Dubstep 的組合）會被歸類到一起，而所有的華語歌曲則歸到了一個大類裏。我曾經也做過一些嘗試，比如把一位歌手同一時期的不同歌曲扔到模型裏找相似，得到的結果完全不一樣，所以你就可以很容易看到歌手在不同時期的變化和成長。</p>
<p>隨手貼兩張截圖：圖一是卷積網絡的框架，圖二是第一層網絡的濾波器。有興趣的同學可以去看一下他針對當時的研究寫的 blog：<a href="http://link.zhihu.com/?target=http%3A//benanne.github.io/2014/08/05/spotify-cnns.html">Recommending music on Spotify with deep learning</a>。可能對於在其他研究領域也有啓發作用。</p>
<p>寫到這裏順便提一下，各位可以猜測一下這位實習生最後去了哪裏？<strong>沒錯，他也加入了 AlphaGo 的發明者所在的公司 &mdash;&mdash;Google Deepmind</strong></p>
<img class="content-image" src="http://pic3.zhimg.com/70/6a4b1a571061efb731772b248f5dee32_b.jpg" alt="">
<img class="content-image" src="http://pic4.zhimg.com/70/001ba6326f7949f50cad9440470fba7f_b.jpg" alt="">
<p>接下來我們來談談人工智能在音樂創作上的研究<strong>，</strong></p>
<p>題主說音樂是一個有限元素的組合創作。 @阿魯卡多 說音樂是無限元素的組合創作。我的理解是，如果單純的認爲音樂是音符的組合而認爲他是有限的，那顯然是非常膚淺的。對於音符組合的模仿作曲其實早在上個世紀 60 年代就開始了 （Daivd Cope 著名的&nbsp;<a href="http://link.zhihu.com/?target=http%3A//artsites.ucsc.edu/faculty/cope/experiments.htm">EMI</a>&nbsp;實驗，通過統計音符概率來模仿古典作曲家進行創作），再進一點的更多所謂的先鋒實驗創作，算法作曲等等也是不勝枚舉。</p>
<p>但是，因爲音樂在配器，編曲，強弱等等其中的變化是無窮多種的。相同的旋律線，在不同的演繹下是兩首完全不同的音樂。然而&hellip;&hellip;</p>
<p>自從我們全面進入數字音樂時代之後，我們就應該清楚地意識到，所有的音樂本質上都是二進制裏的 0 和 1，而對於計算機來說，所有的 0 和 1 的組合，<strong>本質上都是有限的。因爲，人的聽力範圍是有限的。</strong></p>
<img class="content-image" src="http://pic3.zhimg.com/70/4887546122a49b2a690ba73b1244a24e_b.jpg" alt="">
<p>上圖是在語音和音樂領域常用到的語譜圖（Spectorgram）.我們把一首歌的 X 軸當成時間，Y 軸當成短時頻率，顏色的深淺表示頻率分佈上的能量大小。以傳統 44100Hz 爲例的採樣率來看，每一秒鐘在時間上是 44100 個點。那麼一首 5 分鐘的歌曲就有接近 1300 萬個採樣點，每個點又根據採樣精度有多種可能。但是人耳真正能感知的頻率上限和聲音長度都是有限的（24KHz, 0.1s），所以在經過短時傅里葉變換和加窗之後，剩下的數據點其實遠遠小於原採樣點。<strong>無論音色再怎麼豐富多變，情感再怎麼即興，當採樣點的精度大於人耳可分辨度的時候，那麼音頻信號的組合，確實是有限的。只不過這個上限非常之大。</strong></p>
<p>寫到這裏似乎有一點令人沮喪，其實這也是我最近在看了阿爾法狗和小李對決之後一直在思考的問題，人類的音樂創作能力是不是真的在某一天會被機器所取代？我認爲至少在短時間內暫時不會。畢竟從組合的角度來講音頻信號能有的組合還是遠遠大於圍棋的決策樹的，而創作又不是一件有規則的事情，所以很難再沒有範式的情況下自主學習。我在另一個問題 <a href="https://www.zhihu.com/question/38156351/answer/75298929">數字音頻行業目前還有哪些發展前景？</a>也提到了：當前數字音頻最大的兩個問題本質上可能還是要靠深度學習來解決，信號分離和物理建模。前者是將來自不同音軌的混合信號完全分離出來，後者是完全用數字手段來模擬聲學信號，一旦這兩個問題解決了，可能 AI 可以和人類一樣創作的日子也就不那麼遠了。</p>
<p>從這個意義上來講，在短時間內比較靠譜的一個 AI 之於音樂的應用，我認爲還是在編曲上。在我看來，很多工業化成熟的編曲，無論歐美，日韓，港臺還是內地，是有規則在裏面的。無論你是多麼複雜的配器，如果將音色的信號級變化轉換成 midi 和音軌作爲神經網絡的輸入信號，那將大大降低機器學習的複雜度。當然，這並不算是創新，只是讓流水線上的編曲工作者們幹活輕鬆一點罷了（打個比方，沒有不尊敬的意思）。總結一下，如果我們跳出思維定式，而真正從計算機的角度來看待這個問題。其實總有一天，計算機會擁有和人類一樣理解音樂和創作音樂的能力。然而，正如有一位朋友的回答裏提到的那樣，音樂是一種純精神層面的東西，你的目標不是戰勝對手，而是創作出有藝術價值的東西，讓大家產生情感共鳴。所以我希望看到的，不是人工智能一味的模仿時下流行歌手進行創作，一天自動生成 200 萬首神曲。而是在擁有理解和創作能力之後，能夠真正的做出一些有創造性的東西。這個界限很難定義，也許在它到來之前永遠不會有一個正確答案。也許這個道理也同樣適用於其他創作領域比如繪畫，小說，電影等。</p>
<p>只是希望它到來的那一天，我們還是能夠用坦然的心態面對它。&ldquo;<strong>給歲月以文明，而不是給文明以歲月&rdquo;。</strong></p>
</div>
</div>




</div>


</div>
</div>