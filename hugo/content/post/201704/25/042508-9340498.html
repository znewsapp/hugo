+++
date = "2017-04-25T08:00:00"
title = "想更好地和「智能助手」聊天，首先要幹掉屏幕"
titleimage = "https://pic1.zhimg.com/v2-1bdd60afd80b7d7b8204277adc519ddc.jpg"
ga = 042508
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">未來語音技術或者語音智能助手的發展方向是什麼？</h2>

<div class="answer">



<div class="content">
<p><strong><strong>無屏化</strong></strong></p>
<p>語音助手會在越來越多沒有屏幕的設備上出現，或者說語音助手會朝着<strong>純語音（VUI）</strong>的交互方式發展。</p>
<p>首先解釋一下純語音交互。純語音交互指的是我們給語音助手錶達命令、確認、打斷、糾錯等等信息交換的過程完全通過語音來進行，沒有點擊、滑動等交互。</p>
<p>我們先確認一點，屏幕作爲我們現在和智能設備交互的主要渠道，它十分重要。我們也非常習慣這種交互方式。爲了更好地和屏幕進行交互，我們發明了 GUI（<strong>圖形用戶界面</strong>），我們創造了各種各樣的交互，如：點擊、滑動、觸摸、長按等等。我們太習慣，太喜歡這種交互方式了。甚至看到 App 圖標上的小紅點就手癢癢，忍不住去點。</p>
<img class="content-image" src="http://pic3.zhimg.com/70/v2-be1c9915ac1ec7df296d484a8e9b6b7e_b.jpg" alt="">
<p>所以，當我們面對一個帶有語音助手的手機時候，我們會去使用使用語音助手麼？太難了！！</p>
<p>Siri 從 07 年開始開發，10 年被蘋果收購，滿打滿算，十年了。十年後的今天，我們做用戶調研的時候發現，沒有一個受訪者<strong>每天</strong>使用 Siri ，或者更具體是，沒有一位受訪者是<strong>每星期</strong>都使用 Siri 的。</p>
<p>原因很簡單，屏幕上幾乎所有的設計都是在誘惑你去點擊、滑動或者完成更復雜的『手指交互』。</p>
<p>而使用語音助手要求在一個滿是 GUI 的頁面上去使用語音和屏幕進行交互，這樣的錯位讓用戶充滿了割裂感。這樣的割裂感是現在屏幕語音助手無法克服的障礙。</p>
<p>好比說，一個使用屏幕 VUI 的用戶在 VUI 上完成一個『任務』後，可能會去 GUI 上完成 1000 個『任務』。這樣的使用情景下，用戶無法形成 『 與設備對話 』的習慣。</p>
<p>那麼 <strong>無屏化設備 /</strong><strong>弱屏化設備 </strong>是最適合語音助手發展的平臺，比如屏幕很小的手錶、手環。或者連屏幕都沒有的音響、家居助手、智能戒指等等...</p>
<p>這些平臺的可交互性或許比屏幕低，但是這些平臺的語音助手給用戶更多的沉浸感。自然地更容易養成『 與設備對話 』這樣的用戶習慣。</p>
<p>最近幾年來，符合這樣趨勢產品也越來越多了，比如 Alexa、Nest、以及已經被放棄的 Google glass 都屬於這類產品。</p>
<p>所以語音助手的<strong>無屏化</strong>將是趨勢之一。</p>
<p><strong><strong>信息去中心化</strong></strong></p>
<p>這寫到半途，我的一個好友評論說</p>
<blockquote>其實主要還是因爲語音助手不智能，或者說 nlp 做的不好...</blockquote>
<p>這句話其實對了一半，『 不智能 』的確是現狀，但卻不僅僅是 NLP 這一個方面。而更多的是『 語音助手擁有信息的豐富程度 』。</p>
<p><a href="https://link.zhihu.com/?target=https%3A//www.soundhound.com/hound">Hound - Say It. Get it.</a><a href="https://link.zhihu.com/?target=https%3A//www.soundhound.com/hound">Hound - Say It. Get it.</a> 是一家語音助手的公司，他們的產品 demo 視頻非常精彩，如『 告訴我在西雅圖四星到五星的酒店，我想住三個晚上，費用是$150-$250 』這樣的長句。這樣的句子已經能被機器所能『 理解 』，那剩下的問題是如何向用戶提供這些信息？</p>
<p>還是上面的例子，如果需要『 知道 』符合這些條件的酒店信息，首先語音助手得有這些信息。有了信息之後你才能找到『 在西雅圖的 』、『四星到五星的』等等符合這些條件的。現在的做法是用 Yelp Api 或者其他的 Api 來獲取這些信息。那麼一個語音助手擁有多少信息很大程度上依賴於它集成了多少 Api，以及這些 Api 能提供信息的豐富程度。</p>
<p>酒店只是其中一個例子，機票、電影票、日程安排、打車等等所有的這些服務信息幾乎都源於『 非語音助手內 』</p>
<p>這樣的信息，我們稱爲『 非中心信息 』。</p>
<p>回到我們關於『 智能 』的討論，如果今天你問一個語音助手『 幫我找一家酒店 』，回答『 不會 』，有時候並不是語音助手不能理解酒店這個詞，而是他們沒有接入相關的信息，沒有相關的信息可以提供，最後只能去 X 度搜一下。</p>
<p>『 非中心信息 』或者 『 非中心功能 』現在已經是語音助手的大趨勢了。Siri 開放 Api 允許部分開發者接入也是不得已的事情，因爲它根本沒有你微信內、滴滴內的信息，只有開放自己的 Api ，才能爲 Siri 獲得更多的『 非中心信息 』。更有甚者，像是 Api.ai 這樣提供 NLP 的平臺直接以 SAAS 讓開發者來接入。</p>
<p>所以<strong>『 信息去中心化 』</strong>是語音助手的另外一個趨勢。</p>
</div>
</div>




</div>


</div>
</div>