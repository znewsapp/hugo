+++
date = "2017-04-17T07:00:00"
title = "Siri 們的腔調怎麼聽起來那麼「假」？"
titleimage = "https://pic2.zhimg.com/v2-cb85a00d40da3250eedd09f39fb12dd5.jpg"
ga = 041707
+++

<div class="main-wrap content-wrap">
<div class="headline">

<div class="img-place-holder"></div>



</div>

<div class="content-inner">




<div class="question">
<h2 class="question-title">目前，人工智能語音在說中文時的語氣感覺上還比較機械，怎樣使人工智能語音的語氣更自然一些？</h2>

<div class="answer">



<div class="content">
<p>我覺得，與其說發音和語氣說成是分開的兩個過程，不如說是合成目標的兩種層級。發音清晰是首先需要解決的問題，是解決讓用戶聽不聽得懂的問題；而語氣自然不自然則是更高層級的問題，是要使得合成語音聽起來就是一個真人在說話。</p>
<p>目前主流的 TTS 都是數據驅動的，抑揚頓挫的感覺必須從聲優的錄音數據中學習出來。要對語氣進行建模，對錄音數據量的要求一定更高，因爲畢竟變量增加了，要做好，一定需要更多的學習數據。</p>
<p>合成本身是從簡到繁的數據產生問題。僅僅看 TTS 的一頭一尾：輸入的是文本，最終輸出的是語音，這是一種 1 對多的產生式問題。用人來舉例子，一句文字，放在電影劇本中，讓同一個演技派演員來表演，都能用多種表達方式展現給導演。何況現實中，每句話的上下文，說話對象，情緒，場景都會發生變化。因此文字擺出來，可以有各種各樣的表達方式，每一種表達對人來說都是自然的，那讓機器怎麼學？學哪一種呢？</p>
<p>據我所知，現在通常的做法是，訓練時需要先對數據進行更加細緻的描述。訓練數據的標註就是要把發音中的不同語氣現象描述出來，然後再送給模型訓練算法進行學習。 常見的語氣相關的數據描述包含但不限於下面這些：韻律邊界，重音，邊界調，甚至情感。 還有更多的信息甚至是難以客觀描述的，目前的算法只能暫且忽略。</p>
<p>上面講的都是訓練時的工作，在語音被合成的時候，還要從文字中將這些數據描述信息預測出來。告訴 AI 用哪種表達方式來讀，然後才用模型生成對應的語音。預測的不準確通常是造成合成不自然現象的首要原因。</p>
<p>個人認爲，從原理上來說，如果有足夠大的訓練數據，涵蓋了足夠全面的語氣現象，再經過詳細的數據描述，這樣訓練出來的模型是能支持合成各種變化的語氣的。並且隨着各種訓練數據量的增加，能夠有越來越好的自然度。</p>
<p>但是實際情況中，數據的積累是很慢的（畢竟數據標註的難度擺在那裏）。通常的做法是讓錄音包含的語氣逐步進行擴充。首先選擇產品最通用的語氣情感，然後逐步放開限定範圍，增加相應的變化。 對於人工智能助手產品來講，這是一種比較自然的進化過程。</p>
<p>最後，最新的 end－to－end 的 TTS 也許能將數據描述的學習過程直接從未標註數據中學到。將來的數據描述可能不需要像今天這麼細緻，但我覺得很大的可能性是：還是要有一些基礎的描述信息，來輔助機器學習建模。 但是，一旦能夠放鬆對數據描述的依賴，數據量的增加就變得及其容易，合成的性能就能有真正本質性的提升。</p>
<p>ps. 母語比外語更難提高語氣自然度。因爲人類被母語訓練的太熟了，不僅僅能從母語中聽到語義的內容，還能從語氣，節奏中聽出話外的意思。這就造成語音中一點點的語氣不自然都能被母語用戶聽出來。</p>
</div>
</div>




</div>


</div>
</div>